---
title: "[Python 머신러닝] 04-6 XGBoost(eXtra Gradient Boost)"

categories: 
  - PYTHON
tags:
  - [Python, 프로그래밍, 머신러닝, 공부]

toc: true
toc_sticky: true
---

# 분류


## XGBoost(eXtra Gradient Boost)


### XGBoost 개요

항목|설명
---|---
뛰어난 예측 성능|일반적으로 분류와 회귀 영역에서 뛰어난 예측 성능을 발휘한다.
GBM 대비 <br> 빠른 수행 시간|일반적인 GBM은 순차적으로 Weak learner가 가중치를 증감하는 방법으로 학습하기 때문에 전반적으로 속도가 느리다 <br> 하지만 XGBoost는 병렬 수행 및 다양한 기능으로 GBM에 비해 빠른 수행 성능을 보장한다. <br> 아쉽게도 XGBoost가 일반적인 GBM에 수행 시간이 빠르다는 것이지, 다른 머신런이 알고리즘(예를 들어 랜덤 포레스트)에 비해서 빠르다는 의미는 아니다.
과적합 규제 <br> (Reglarization)|표준 GBM의 경우 과적합 규제 기능이 없으나 XGBoost는 자체에 과적합 구제 기능으로 과적합에 좀 더 강한 내구성을 가질 수 있다.
Tree pruning <br> (나무 가지치기)|일반적으로 GBM은 분할 시 부정 손실이 발생하면 분할을 더 이상 수행하지 않지만, 이러한 방식도 자칫 지나치게 많은 분할을 발생할 수도 있다. <br> 다른 GBM과 마찬가지로 XGBoost도 max_depth 파라미터로 분할 깊이를 조저하기도 하지만, tree pruning으로 더 이상 긍정 이득이 없는 분할을 가지치기 해서 분할 수를 더 줄이는 추가적인 장점을 가지고 있다.
자체 내장된 <br> 교차 검증|XGBoost는 반복 수행 시마다 내부적으로 학습 데이터 세트와 평가 데이터 세트에 대한 교차 검증을 수행해 최적화된 반복 수행 횟수를 가질 수 있다. <br> 지정된 반복 횟수가 아니라 교차 검증을 통해 평가 데이터 세트의 평가 값이 최적화 되면 반복을 중간에 멈출 수 있는 조기 중단 기능이 있다.
결손값 자체 처리|XGBoost는 결손값을 자체 처리할 수 있는 기능을 가지고 있다.


#### XGBoost 파이썬 구현
1. C/C++ Native Module 
    - XGBoost는 최초 c/c++로 작성됨  

2. 파이썬 Wrapper 
    - c/c++ 모듈을 호출하는 파이선 Wrapper  

3. 사이킷런 Wrapper 
    - 사이킷런 프레임과 통합 될 수 있는 파이썬 Wrapper Class 지원 (XGBClassifier, XGBRegressor)
    - 학습과 예측을 다른 사이킷런 API와 동일하게 fit()과 predict()로 수행
    - GridSearchCV와 같은 다른 사이킷런 모듈과 같이 사용 가능


#### XGBoost 파이썬 래퍼와 사이킷런 래퍼 API 비교


항목|파이썬 Wrapper|사이킷런 Wrapper
---|---|---
사용 모듈|from xgboost as xgb|from xgboost import XGBClassifier
학습용과 테스트용 <br> 데이터 세트|DMatrix 객체를 별도 생성 <br> train = xgb.DMatrix(data=X_train, label=y_train) <br> DMatrix 생성자로 피처 데이터 세트와 레이블 데이터 세트를 입력|넘파이나 판다스를 이용
학습 API|Xgb_model=xgb.train() <br> Xgb_model은 학습된 객체를 반환 받음|XGBClassifer.fit()
예측 API|xbg.train()으로 학습된 객체에서 predict() 호출. 즉 Xgb_model.predict() <br> 이때 반환 결과는 예측 결과가 아니라 예측 결과를 추정하는 확률값 반환|XGBClassifer.predict() <br> 예측 결과값 반환
피처 중요도 시각화|plot_importance() 함수 이용|plot_importance() 함수 이용


### XGBoost 설치하기


아나콘다 Command 창을 연 뒤 `conda install -c anaconda py-xgboost` 명령어를 입력한다.

#### <실습>


```python
#XGBoost 버전 확인
import xgboost

print(xgboost.__version__)
```

    1.5.0



### 파이썬 래퍼 XGBoost 하이퍼 파라미터


#### XGBoost 파이썬 래퍼와 사이킷런 래퍼 하이퍼 파라미터 비교

파이썬 Wrapper|사이킷런 Wrapper|하이퍼 파라미터 설명
---|---|---
eta|learning_rate|GBM의 학습률(learning rate)과 같은 파라미터이다. 0에서 1 사이의 값을 지정하며 부스팅 스텝을 반복적으로 수행할 때 업데이트되는 학습률 값 <br> 파이썬 래퍼 기반의 xgboost를 이용할 경우 디폴트는 0.3, 사이킷런 래퍼 클래스를 이용할 경우 eta는 learning_rate 파라미터로 대체되며, 디폴트는 0.1이다.
num_boost_rounds|n_estimators|사이킷런 앙상블의 n_estimators와 동일. 약한 학습기의 개수(반복 수행 횟수)
min_child_weight|min_child_weight|결정 트리의 min_child_leaf와 유사. 과적합 조절용
max_depth|max_depth|결정 트리의 max_depth와 동일. 트리의 최대 깊이
sub_sample|subsample|GBM의 subsample과 동일. 트리가 커져서 과적합되는 것을 제어하기 위해 데이터를 샘플링하는 비율을 지정한다. <br> sub_sample=0.5로 지정하면 전체 데이터의 절반을 트리를 생성하는 데 사용한다. <br> 0에서 1 사이의 값이 가능하나 일반적으로 0.5~1 사이의 값을 사용한다.
lambda|reg_lambda|L2 규제(Regularization) 적용 값이다. 기본값은 1 <br> 값이 클 수록 구제 값이 커지며 과적합 제어 효과가 있다.
alpha|reg_alpha|L1 규제(Regularization) 적용 값이다. 기본값은 0 <br> 값이 클 수록 규제 값이 커지며 과적합 제어 효과가 있다.
colsample_bytree|colsample_bytree|GBM의 max_features와 유사하다. <br> 트리 생성에 필요한 피처(칼럼)를 임의로 샘플링하는 데 사용된다. <br> 매우 많은 피처가 있는 경우 과적합을 조정하는 데 적용한다.
scale_pos_weight|scale_pos_weight|특정 값으로 치우친 비대칭한 클래스로 구성된 데이터 세트의 균형을 유지하기 위한 파라미터이다. 기본값은 1
gamma|gamma|트리의 리프 노드를 추가적으로 나눌지를결정할 최소 손실 감소 값이다. <br> 해당 값보다 큰 손실(loss)이 감소된 경우에 리프 노드를 분리한다. 값이 클수록 과적합 감소 효과가 있다.


사이킷런 Wrapper의 경우 GBM에 동일한 하이퍼 파라미터가 있다면 이를 사용하고 그렇지 않다면 파이썬 Wrapper의 하이퍼 파라미터를 사용


#### XGBoost 조기 중단 기능 (Early Stopping)
- XGBoost는 특정 반복 횟수 만큼 더 이상 비용함수가 감소하지 않으면 지정된 반복횟수를 다 완료하지 않고 수행을 종료할 수 있음
- 학습을위한 시간을 다축 시킬 수 있음. 특히 최적화 튜닝 단계에서 적절하게 사용 가능
- 너무 반복 횟수를 단축할 경우 예측 성능 최적화가 안 된 상태에서 학습이 종료 될 수 있으므로 유의 필요
- 조기 중단 설정을 위한 주요 파라미터
  - early_stopping_rounds: 더 이상 비용 평가 지표가 감소하지 않는 최대 반복횟수
  - eval_metric: 반복 수행 시 사용하는 비용 평가 지표
  - eval_set: 평가를 수행하는 별도의 검증 데이터 세트. 일반적으로 검증 데이터 세트에서 반복적으로 비용 감소 성능 평가



### 파이썬 래퍼 XGBoost 적용 - 위스콘신 유방암 예측

#### <실습>


**데이터 세트 로딩 하기**


```python
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
# xgboost 패키지 로딩하기
import xgboost as xgb
from xgboost import plot_importance

import warnings
warnings.filterwarnings('ignore')

dataset = load_breast_cancer()
features= dataset.data
labels = dataset.target

cancer_df = pd.DataFrame(data=features, columns=dataset.feature_names)
cancer_df['target']= labels
cancer_df.head(3)

```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean radius</th>
      <th>mean texture</th>
      <th>mean perimeter</th>
      <th>mean area</th>
      <th>mean smoothness</th>
      <th>mean compactness</th>
      <th>mean concavity</th>
      <th>mean concave points</th>
      <th>mean symmetry</th>
      <th>mean fractal dimension</th>
      <th>...</th>
      <th>worst texture</th>
      <th>worst perimeter</th>
      <th>worst area</th>
      <th>worst smoothness</th>
      <th>worst compactness</th>
      <th>worst concavity</th>
      <th>worst concave points</th>
      <th>worst symmetry</th>
      <th>worst fractal dimension</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17.99</td>
      <td>10.38</td>
      <td>122.8</td>
      <td>1001.0</td>
      <td>0.11840</td>
      <td>0.27760</td>
      <td>0.3001</td>
      <td>0.14710</td>
      <td>0.2419</td>
      <td>0.07871</td>
      <td>...</td>
      <td>17.33</td>
      <td>184.6</td>
      <td>2019.0</td>
      <td>0.1622</td>
      <td>0.6656</td>
      <td>0.7119</td>
      <td>0.2654</td>
      <td>0.4601</td>
      <td>0.11890</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>20.57</td>
      <td>17.77</td>
      <td>132.9</td>
      <td>1326.0</td>
      <td>0.08474</td>
      <td>0.07864</td>
      <td>0.0869</td>
      <td>0.07017</td>
      <td>0.1812</td>
      <td>0.05667</td>
      <td>...</td>
      <td>23.41</td>
      <td>158.8</td>
      <td>1956.0</td>
      <td>0.1238</td>
      <td>0.1866</td>
      <td>0.2416</td>
      <td>0.1860</td>
      <td>0.2750</td>
      <td>0.08902</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>19.69</td>
      <td>21.25</td>
      <td>130.0</td>
      <td>1203.0</td>
      <td>0.10960</td>
      <td>0.15990</td>
      <td>0.1974</td>
      <td>0.12790</td>
      <td>0.2069</td>
      <td>0.05999</td>
      <td>...</td>
      <td>25.53</td>
      <td>152.5</td>
      <td>1709.0</td>
      <td>0.1444</td>
      <td>0.4245</td>
      <td>0.4504</td>
      <td>0.2430</td>
      <td>0.3613</td>
      <td>0.08758</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>3 rows × 31 columns</p>
</div>




```python
print(dataset.target_names)
print(cancer_df['target'].value_counts())
```

    ['malignant' 'benign']
    1    357
    0    212
    Name: target, dtype: int64
    


```python
# cancer_df에서 feature용 DataFrame과 Label용 Series 객체 추출
# 맨 마지막 칼럼이 Label이므로 Feature용 DataFrame은 cancer_df의 첫번째 칼럼에서 맨 마지막 두번째 컬럼까지를 :-1 슬라이싱으로 추출.
X_features = cancer_df.iloc[:, :-1]
y_label = cancer_df.iloc[:, -1]

# 전체 데이터 중 80%는 학습용 데이터, 20%는 테스트용 데이터 추출
X_train, X_test, y_train, y_test=train_test_split(X_features, y_label, test_size=0.2, random_state=156 )

# 위에서 만든 X_train, y_train을 다시 쪼개서 90%는 학습과 10%는 검증용 데이터로 분리 
X_tr, X_val, y_tr, y_val= train_test_split(X_train, y_train, test_size=0.1, random_state=156 )

print(X_train.shape , X_test.shape)
print(X_tr.shape, X_val.shape)
```

    (455, 30) (114, 30)
    (409, 30) (46, 30)
    

**학습과 예측 데이터 세트를 DMatrix로 변환**
* DMatrix는 넘파이 array, DataFrame에서도 변환 가능


```python
# 만약 구버전 XGBoost에서 DataFrame으로 DMatrix 생성이 안될 경우 X_train.values로 넘파이 변환. 
# 학습, 검증, 테스트용 DMatrix를 생성. 
dtr = xgb.DMatrix(data=X_tr, label=y_tr)
dval = xgb.DMatrix(data=X_val, label=y_val)
dtest = xgb.DMatrix(data=X_test , label=y_test)
```

**하이퍼 파라미터 설정**


```python
params = { 'max_depth':3,
           'eta': 0.05,
           'objective':'binary:logistic',
           'eval_metric':'logloss'
        }
num_rounds = 400
```

**주어진 하이퍼 파라미터와 early stopping 파라미터를 train( ) 함수의 파라미터로 전달하고 학습**


```python
# 학습 데이터 셋은 'train' 또는 평가 데이터 셋은 'eval' 로 명기합니다. 
eval_list = [(dtr,'train'),(dval,'eval')] # 또는 eval_list = [(dval,'eval')] 만 명기해도 무방. 

# 하이퍼 파라미터와 early stopping 파라미터를 train( ) 함수의 파라미터로 전달
xgb_model = xgb.train(params = params , dtrain=dtr , num_boost_round=num_rounds , \
                      early_stopping_rounds=50, evals=eval_list )
```

    [0]	train-logloss:0.65016	eval-logloss:0.66183
    [1]	train-logloss:0.61131	eval-logloss:0.63609
    [2]	train-logloss:0.57563	eval-logloss:0.61144
    [3]	train-logloss:0.54310	eval-logloss:0.59204
    [4]	train-logloss:0.51323	eval-logloss:0.57329
    [5]	train-logloss:0.48447	eval-logloss:0.55037
    [6]	train-logloss:0.45796	eval-logloss:0.52929
    [7]	train-logloss:0.43436	eval-logloss:0.51534
    [8]	train-logloss:0.41150	eval-logloss:0.49718
    [9]	train-logloss:0.39027	eval-logloss:0.48154
    [10]	train-logloss:0.37128	eval-logloss:0.46990
    [11]	train-logloss:0.35254	eval-logloss:0.45474
    [12]	train-logloss:0.33528	eval-logloss:0.44229
    [13]	train-logloss:0.31893	eval-logloss:0.42961
    [14]	train-logloss:0.30439	eval-logloss:0.42065
    [15]	train-logloss:0.29000	eval-logloss:0.40958
    [16]	train-logloss:0.27651	eval-logloss:0.39887
    [17]	train-logloss:0.26389	eval-logloss:0.39050
    [18]	train-logloss:0.25210	eval-logloss:0.38254
    [19]	train-logloss:0.24123	eval-logloss:0.37393
    [20]	train-logloss:0.23076	eval-logloss:0.36789
    [21]	train-logloss:0.22091	eval-logloss:0.36017
    [22]	train-logloss:0.21155	eval-logloss:0.35421
    [23]	train-logloss:0.20263	eval-logloss:0.34683
    [24]	train-logloss:0.19434	eval-logloss:0.34111
    [25]	train-logloss:0.18637	eval-logloss:0.33634
    [26]	train-logloss:0.17875	eval-logloss:0.33082
    [27]	train-logloss:0.17167	eval-logloss:0.32675
    [28]	train-logloss:0.16481	eval-logloss:0.32099
    [29]	train-logloss:0.15835	eval-logloss:0.31671
    [30]	train-logloss:0.15225	eval-logloss:0.31277
    [31]	train-logloss:0.14650	eval-logloss:0.30882
    [32]	train-logloss:0.14102	eval-logloss:0.30437
    [33]	train-logloss:0.13590	eval-logloss:0.30103
    [34]	train-logloss:0.13109	eval-logloss:0.29794
    [35]	train-logloss:0.12647	eval-logloss:0.29499
    [36]	train-logloss:0.12197	eval-logloss:0.29295
    [37]	train-logloss:0.11784	eval-logloss:0.29043
    [38]	train-logloss:0.11379	eval-logloss:0.28927
    [39]	train-logloss:0.10994	eval-logloss:0.28578
    [40]	train-logloss:0.10638	eval-logloss:0.28364
    [41]	train-logloss:0.10302	eval-logloss:0.28183
    [42]	train-logloss:0.09963	eval-logloss:0.28005
    [43]	train-logloss:0.09649	eval-logloss:0.27972
    [44]	train-logloss:0.09359	eval-logloss:0.27744
    [45]	train-logloss:0.09080	eval-logloss:0.27542
    [46]	train-logloss:0.08807	eval-logloss:0.27504
    [47]	train-logloss:0.08541	eval-logloss:0.27458
    [48]	train-logloss:0.08299	eval-logloss:0.27348
    [49]	train-logloss:0.08035	eval-logloss:0.27247
    [50]	train-logloss:0.07786	eval-logloss:0.27163
    [51]	train-logloss:0.07550	eval-logloss:0.27094
    [52]	train-logloss:0.07344	eval-logloss:0.26967
    [53]	train-logloss:0.07147	eval-logloss:0.27008
    [54]	train-logloss:0.06964	eval-logloss:0.26890
    [55]	train-logloss:0.06766	eval-logloss:0.26854
    [56]	train-logloss:0.06592	eval-logloss:0.26900
    [57]	train-logloss:0.06433	eval-logloss:0.26790
    [58]	train-logloss:0.06259	eval-logloss:0.26663
    [59]	train-logloss:0.06107	eval-logloss:0.26743
    [60]	train-logloss:0.05957	eval-logloss:0.26610
    [61]	train-logloss:0.05817	eval-logloss:0.26644
    [62]	train-logloss:0.05691	eval-logloss:0.26673
    [63]	train-logloss:0.05550	eval-logloss:0.26550
    [64]	train-logloss:0.05422	eval-logloss:0.26443
    [65]	train-logloss:0.05311	eval-logloss:0.26500
    [66]	train-logloss:0.05207	eval-logloss:0.26591
    [67]	train-logloss:0.05093	eval-logloss:0.26501
    [68]	train-logloss:0.04976	eval-logloss:0.26435
    [69]	train-logloss:0.04872	eval-logloss:0.26360
    [70]	train-logloss:0.04776	eval-logloss:0.26319
    [71]	train-logloss:0.04680	eval-logloss:0.26255
    [72]	train-logloss:0.04580	eval-logloss:0.26204
    [73]	train-logloss:0.04484	eval-logloss:0.26254
    [74]	train-logloss:0.04388	eval-logloss:0.26289
    [75]	train-logloss:0.04309	eval-logloss:0.26249
    [76]	train-logloss:0.04224	eval-logloss:0.26217
    [77]	train-logloss:0.04133	eval-logloss:0.26166
    [78]	train-logloss:0.04050	eval-logloss:0.26179
    [79]	train-logloss:0.03967	eval-logloss:0.26103
    [80]	train-logloss:0.03877	eval-logloss:0.26094
    [81]	train-logloss:0.03806	eval-logloss:0.26148
    [82]	train-logloss:0.03740	eval-logloss:0.26054
    [83]	train-logloss:0.03676	eval-logloss:0.25967
    [84]	train-logloss:0.03605	eval-logloss:0.25905
    [85]	train-logloss:0.03545	eval-logloss:0.26007
    [86]	train-logloss:0.03488	eval-logloss:0.25984
    [87]	train-logloss:0.03425	eval-logloss:0.25933
    [88]	train-logloss:0.03361	eval-logloss:0.25932
    [89]	train-logloss:0.03311	eval-logloss:0.26002
    [90]	train-logloss:0.03260	eval-logloss:0.25936
    [91]	train-logloss:0.03202	eval-logloss:0.25886
    [92]	train-logloss:0.03152	eval-logloss:0.25918
    [93]	train-logloss:0.03107	eval-logloss:0.25865
    [94]	train-logloss:0.03049	eval-logloss:0.25951
    [95]	train-logloss:0.03007	eval-logloss:0.26091
    [96]	train-logloss:0.02963	eval-logloss:0.26014
    [97]	train-logloss:0.02913	eval-logloss:0.25974
    [98]	train-logloss:0.02866	eval-logloss:0.25937
    [99]	train-logloss:0.02829	eval-logloss:0.25893
    [100]	train-logloss:0.02789	eval-logloss:0.25928
    [101]	train-logloss:0.02751	eval-logloss:0.25955
    [102]	train-logloss:0.02714	eval-logloss:0.25901
    [103]	train-logloss:0.02668	eval-logloss:0.25991
    [104]	train-logloss:0.02634	eval-logloss:0.25950
    [105]	train-logloss:0.02594	eval-logloss:0.25924
    [106]	train-logloss:0.02556	eval-logloss:0.25901
    [107]	train-logloss:0.02522	eval-logloss:0.25738
    [108]	train-logloss:0.02492	eval-logloss:0.25702
    [109]	train-logloss:0.02453	eval-logloss:0.25789
    [110]	train-logloss:0.02418	eval-logloss:0.25770
    [111]	train-logloss:0.02384	eval-logloss:0.25842
    [112]	train-logloss:0.02356	eval-logloss:0.25810
    [113]	train-logloss:0.02322	eval-logloss:0.25848
    [114]	train-logloss:0.02290	eval-logloss:0.25833
    [115]	train-logloss:0.02260	eval-logloss:0.25820
    [116]	train-logloss:0.02229	eval-logloss:0.25905
    [117]	train-logloss:0.02204	eval-logloss:0.25878
    [118]	train-logloss:0.02176	eval-logloss:0.25728
    [119]	train-logloss:0.02149	eval-logloss:0.25722
    [120]	train-logloss:0.02119	eval-logloss:0.25764
    [121]	train-logloss:0.02095	eval-logloss:0.25761
    [122]	train-logloss:0.02067	eval-logloss:0.25832
    [123]	train-logloss:0.02045	eval-logloss:0.25808
    [124]	train-logloss:0.02023	eval-logloss:0.25855
    [125]	train-logloss:0.01998	eval-logloss:0.25714
    [126]	train-logloss:0.01973	eval-logloss:0.25587
    [127]	train-logloss:0.01946	eval-logloss:0.25640
    [128]	train-logloss:0.01927	eval-logloss:0.25685
    [129]	train-logloss:0.01908	eval-logloss:0.25665
    [130]	train-logloss:0.01886	eval-logloss:0.25712
    [131]	train-logloss:0.01863	eval-logloss:0.25609
    [132]	train-logloss:0.01839	eval-logloss:0.25649
    [133]	train-logloss:0.01816	eval-logloss:0.25789
    [134]	train-logloss:0.01802	eval-logloss:0.25811
    [135]	train-logloss:0.01785	eval-logloss:0.25794
    [136]	train-logloss:0.01763	eval-logloss:0.25876
    [137]	train-logloss:0.01748	eval-logloss:0.25884
    [138]	train-logloss:0.01732	eval-logloss:0.25867
    [139]	train-logloss:0.01719	eval-logloss:0.25876
    [140]	train-logloss:0.01696	eval-logloss:0.25987
    [141]	train-logloss:0.01681	eval-logloss:0.25960
    [142]	train-logloss:0.01669	eval-logloss:0.25982
    [143]	train-logloss:0.01656	eval-logloss:0.25992
    [144]	train-logloss:0.01638	eval-logloss:0.26035
    [145]	train-logloss:0.01623	eval-logloss:0.26055
    [146]	train-logloss:0.01606	eval-logloss:0.26092
    [147]	train-logloss:0.01589	eval-logloss:0.26137
    [148]	train-logloss:0.01572	eval-logloss:0.25999
    [149]	train-logloss:0.01557	eval-logloss:0.26028
    [150]	train-logloss:0.01546	eval-logloss:0.26048
    [151]	train-logloss:0.01531	eval-logloss:0.26142
    [152]	train-logloss:0.01515	eval-logloss:0.26188
    [153]	train-logloss:0.01501	eval-logloss:0.26227
    [154]	train-logloss:0.01486	eval-logloss:0.26287
    [155]	train-logloss:0.01476	eval-logloss:0.26299
    [156]	train-logloss:0.01461	eval-logloss:0.26346
    [157]	train-logloss:0.01448	eval-logloss:0.26379
    [158]	train-logloss:0.01434	eval-logloss:0.26306
    [159]	train-logloss:0.01424	eval-logloss:0.26237
    [160]	train-logloss:0.01410	eval-logloss:0.26251
    [161]	train-logloss:0.01401	eval-logloss:0.26265
    [162]	train-logloss:0.01392	eval-logloss:0.26264
    [163]	train-logloss:0.01380	eval-logloss:0.26250
    [164]	train-logloss:0.01372	eval-logloss:0.26264
    [165]	train-logloss:0.01359	eval-logloss:0.26255
    [166]	train-logloss:0.01350	eval-logloss:0.26188
    [167]	train-logloss:0.01342	eval-logloss:0.26203
    [168]	train-logloss:0.01331	eval-logloss:0.26190
    [169]	train-logloss:0.01319	eval-logloss:0.26184
    [170]	train-logloss:0.01312	eval-logloss:0.26133
    [171]	train-logloss:0.01304	eval-logloss:0.26148
    [172]	train-logloss:0.01297	eval-logloss:0.26157
    [173]	train-logloss:0.01285	eval-logloss:0.26253
    [174]	train-logloss:0.01278	eval-logloss:0.26229
    [175]	train-logloss:0.01267	eval-logloss:0.26086
    [176]	train-logloss:0.01258	eval-logloss:0.26103
    

**predict()를 통해 예측 확률값을 반환하고 예측 값으로 변환**


```python
pred_probs = xgb_model.predict(dtest)
print('predict( ) 수행 결과값을 10개만 표시, 예측 확률 값으로 표시됨')
print(np.round(pred_probs[:10],3))

# 예측 확률이 0.5 보다 크면 1 , 그렇지 않으면 0 으로 예측값 결정하여 List 객체인 preds에 저장 
preds = [ 1 if x > 0.5 else 0 for x in pred_probs ]
print('예측값 10개만 표시:',preds[:10])
```

    predict( ) 수행 결과값을 10개만 표시, 예측 확률 값으로 표시됨
    [0.845 0.008 0.68  0.081 0.975 0.999 0.998 0.998 0.996 0.001]
    예측값 10개만 표시: [1, 0, 1, 0, 1, 1, 1, 1, 1, 0]
    


```python
pred_probs
```




    array([0.8447872 , 0.00842587, 0.6796298 , 0.08113331, 0.9751338 ,
           0.9988939 , 0.9983084 , 0.9980654 , 0.99637896, 0.00138468,
           0.00252283, 0.00154995, 0.99780875, 0.99829525, 0.99691856,
           0.9965521 , 0.99120796, 0.9982718 , 0.9970682 , 0.9978916 ,
           0.00202923, 0.10774372, 0.00137198, 0.9989255 , 0.00107862,
           0.7800014 , 0.00295459, 0.00154995, 0.9966723 , 0.05379276,
           0.958738  , 0.00149019, 0.9700533 , 0.8656249 , 0.00678389,
           0.00140975, 0.97810876, 0.99713576, 0.24059245, 0.9972307 ,
           0.35760084, 0.99708337, 0.9919429 , 0.99659145, 0.9962838 ,
           0.9179466 , 0.036952  , 0.997417  , 0.99325067, 0.99804085,
           0.99648905, 0.00236221, 0.9979361 , 0.99784875, 0.9960328 ,
           0.99391055, 0.9984106 , 0.99635327, 0.9967404 , 0.896291  ,
           0.9967794 , 0.9520696 , 0.00349248, 0.00202715, 0.9980167 ,
           0.98225844, 0.00349248, 0.99056447, 0.9972249 , 0.9978916 ,
           0.00297725, 0.99731344, 0.00163038, 0.98887384, 0.9962419 ,
           0.00137198, 0.9985329 , 0.9985329 , 0.99858946, 0.00131184,
           0.00139682, 0.93810165, 0.9969139 , 0.99748176, 0.992568  ,
           0.9906398 , 0.9914522 , 0.9930942 , 0.9830724 , 0.00137198,
           0.19445673, 0.99830306, 0.00650652, 0.00560008, 0.99777275,
           0.00793959, 0.02962515, 0.99509096, 0.00236221, 0.78849   ,
           0.00614955, 0.00250252, 0.99592257, 0.99598455, 0.6040961 ,
           0.9969748 , 0.99688077, 0.8580849 , 0.9966723 , 0.9985133 ,
           0.6028515 , 0.97962165, 0.99558735, 0.9978284 ], dtype=float32)



**get_clf_eval( )을 통해 예측 평가**


```python
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import f1_score, roc_auc_score

def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix( y_test, pred)
    accuracy = accuracy_score(y_test , pred)
    precision = precision_score(y_test , pred)
    recall = recall_score(y_test , pred)
    f1 = f1_score(y_test,pred)
    # ROC-AUC 추가 
    roc_auc = roc_auc_score(y_test, pred_proba)
    print('오차 행렬')
    print(confusion)
    # ROC-AUC print 추가
    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\
    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))
```


```python
get_clf_eval(y_test , preds, pred_probs)
```

    오차 행렬
    [[34  3]
     [ 2 75]]
    정확도: 0.9561, 정밀도: 0.9615, 재현율: 0.9740,    F1: 0.9677, AUC:0.9937
    

**Feature Importance 시각화**


```python
import matplotlib.pyplot as plt
%matplotlib inline

fig, ax = plt.subplots(figsize=(10, 12))
plot_importance(xgb_model, ax=ax)
```




    <AxesSubplot:title={'center':'Feature importance'}, xlabel='F score', ylabel='Features'>




    
![png](output_21_1.png)
    


### 사이킷런 Wrapper XGBoost 개요 및 적용 

**사이킷런 래퍼 클래스 임포트, 학습 및 예측**


```python
# 사이킷런 래퍼 XGBoost 클래스인 XGBClassifier 임포트
from xgboost import XGBClassifier

# Warning 메시지를 없애기 위해 eval_metric 값을 XGBClassifier 생성 인자로 입력. 미 입력해도 수행에 문제 없음.   
xgb_wrapper = XGBClassifier(n_estimators=400, learning_rate=0.05, max_depth=3, eval_metric='logloss')
xgb_wrapper.fit(X_train, y_train, verbose=True)
w_preds = xgb_wrapper.predict(X_test)
w_pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1]
```


```python
get_clf_eval(y_test , w_preds, w_pred_proba)
```

    오차 행렬
    [[34  3]
     [ 1 76]]
    정확도: 0.9649, 정밀도: 0.9620, 재현율: 0.9870,    F1: 0.9744, AUC:0.9954
    

**early stopping을 50으로 설정하고 재 학습/예측/평가**


```python
from xgboost import XGBClassifier

xgb_wrapper = XGBClassifier(n_estimators=400, learning_rate=0.05, max_depth=3)
evals = [(X_tr, y_tr), (X_val, y_val)]
xgb_wrapper.fit(X_tr, y_tr, early_stopping_rounds=50, eval_metric="logloss", 
                eval_set=evals, verbose=True)

ws50_preds = xgb_wrapper.predict(X_test)
ws50_pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1]
```

    [0]	validation_0-logloss:0.65016	validation_1-logloss:0.66183
    [1]	validation_0-logloss:0.61131	validation_1-logloss:0.63609
    [2]	validation_0-logloss:0.57563	validation_1-logloss:0.61144
    [3]	validation_0-logloss:0.54310	validation_1-logloss:0.59204
    [4]	validation_0-logloss:0.51323	validation_1-logloss:0.57329
    [5]	validation_0-logloss:0.48447	validation_1-logloss:0.55037
    [6]	validation_0-logloss:0.45796	validation_1-logloss:0.52929
    [7]	validation_0-logloss:0.43436	validation_1-logloss:0.51534
    [8]	validation_0-logloss:0.41150	validation_1-logloss:0.49718
    [9]	validation_0-logloss:0.39027	validation_1-logloss:0.48154
    [10]	validation_0-logloss:0.37128	validation_1-logloss:0.46990
    [11]	validation_0-logloss:0.35254	validation_1-logloss:0.45474
    [12]	validation_0-logloss:0.33528	validation_1-logloss:0.44229
    [13]	validation_0-logloss:0.31893	validation_1-logloss:0.42961
    [14]	validation_0-logloss:0.30439	validation_1-logloss:0.42065
    [15]	validation_0-logloss:0.29000	validation_1-logloss:0.40958
    [16]	validation_0-logloss:0.27651	validation_1-logloss:0.39887
    [17]	validation_0-logloss:0.26389	validation_1-logloss:0.39050
    [18]	validation_0-logloss:0.25210	validation_1-logloss:0.38254
    [19]	validation_0-logloss:0.24123	validation_1-logloss:0.37393
    [20]	validation_0-logloss:0.23076	validation_1-logloss:0.36789
    [21]	validation_0-logloss:0.22091	validation_1-logloss:0.36017
    [22]	validation_0-logloss:0.21155	validation_1-logloss:0.35421
    [23]	validation_0-logloss:0.20263	validation_1-logloss:0.34683
    [24]	validation_0-logloss:0.19434	validation_1-logloss:0.34111
    [25]	validation_0-logloss:0.18637	validation_1-logloss:0.33634
    [26]	validation_0-logloss:0.17875	validation_1-logloss:0.33082
    [27]	validation_0-logloss:0.17167	validation_1-logloss:0.32675
    [28]	validation_0-logloss:0.16481	validation_1-logloss:0.32099
    [29]	validation_0-logloss:0.15835	validation_1-logloss:0.31671
    [30]	validation_0-logloss:0.15225	validation_1-logloss:0.31277
    [31]	validation_0-logloss:0.14650	validation_1-logloss:0.30882
    [32]	validation_0-logloss:0.14102	validation_1-logloss:0.30437
    [33]	validation_0-logloss:0.13590	validation_1-logloss:0.30103
    [34]	validation_0-logloss:0.13109	validation_1-logloss:0.29794
    [35]	validation_0-logloss:0.12647	validation_1-logloss:0.29499
    [36]	validation_0-logloss:0.12197	validation_1-logloss:0.29295
    [37]	validation_0-logloss:0.11784	validation_1-logloss:0.29043
    [38]	validation_0-logloss:0.11379	validation_1-logloss:0.28927
    [39]	validation_0-logloss:0.10994	validation_1-logloss:0.28578
    [40]	validation_0-logloss:0.10638	validation_1-logloss:0.28364
    [41]	validation_0-logloss:0.10302	validation_1-logloss:0.28183
    [42]	validation_0-logloss:0.09963	validation_1-logloss:0.28005
    [43]	validation_0-logloss:0.09649	validation_1-logloss:0.27972
    [44]	validation_0-logloss:0.09359	validation_1-logloss:0.27744
    [45]	validation_0-logloss:0.09080	validation_1-logloss:0.27542
    [46]	validation_0-logloss:0.08807	validation_1-logloss:0.27504
    [47]	validation_0-logloss:0.08541	validation_1-logloss:0.27458
    [48]	validation_0-logloss:0.08299	validation_1-logloss:0.27348
    [49]	validation_0-logloss:0.08035	validation_1-logloss:0.27247
    [50]	validation_0-logloss:0.07786	validation_1-logloss:0.27163
    [51]	validation_0-logloss:0.07550	validation_1-logloss:0.27094
    [52]	validation_0-logloss:0.07344	validation_1-logloss:0.26967
    [53]	validation_0-logloss:0.07147	validation_1-logloss:0.27008
    [54]	validation_0-logloss:0.06964	validation_1-logloss:0.26890
    [55]	validation_0-logloss:0.06766	validation_1-logloss:0.26854
    [56]	validation_0-logloss:0.06592	validation_1-logloss:0.26900
    [57]	validation_0-logloss:0.06433	validation_1-logloss:0.26790
    [58]	validation_0-logloss:0.06259	validation_1-logloss:0.26663
    [59]	validation_0-logloss:0.06107	validation_1-logloss:0.26743
    [60]	validation_0-logloss:0.05957	validation_1-logloss:0.26610
    [61]	validation_0-logloss:0.05817	validation_1-logloss:0.26644
    [62]	validation_0-logloss:0.05691	validation_1-logloss:0.26673
    [63]	validation_0-logloss:0.05550	validation_1-logloss:0.26550
    [64]	validation_0-logloss:0.05422	validation_1-logloss:0.26443
    [65]	validation_0-logloss:0.05311	validation_1-logloss:0.26500
    [66]	validation_0-logloss:0.05207	validation_1-logloss:0.26591
    [67]	validation_0-logloss:0.05093	validation_1-logloss:0.26501
    [68]	validation_0-logloss:0.04976	validation_1-logloss:0.26435
    [69]	validation_0-logloss:0.04872	validation_1-logloss:0.26360
    [70]	validation_0-logloss:0.04776	validation_1-logloss:0.26319
    [71]	validation_0-logloss:0.04680	validation_1-logloss:0.26255
    [72]	validation_0-logloss:0.04580	validation_1-logloss:0.26204
    [73]	validation_0-logloss:0.04484	validation_1-logloss:0.26254
    [74]	validation_0-logloss:0.04388	validation_1-logloss:0.26289
    [75]	validation_0-logloss:0.04309	validation_1-logloss:0.26249
    [76]	validation_0-logloss:0.04224	validation_1-logloss:0.26217
    [77]	validation_0-logloss:0.04133	validation_1-logloss:0.26166
    [78]	validation_0-logloss:0.04050	validation_1-logloss:0.26179
    [79]	validation_0-logloss:0.03967	validation_1-logloss:0.26103
    [80]	validation_0-logloss:0.03877	validation_1-logloss:0.26094
    [81]	validation_0-logloss:0.03806	validation_1-logloss:0.26148
    [82]	validation_0-logloss:0.03740	validation_1-logloss:0.26054
    [83]	validation_0-logloss:0.03676	validation_1-logloss:0.25967
    [84]	validation_0-logloss:0.03605	validation_1-logloss:0.25905
    [85]	validation_0-logloss:0.03545	validation_1-logloss:0.26007
    [86]	validation_0-logloss:0.03488	validation_1-logloss:0.25984
    [87]	validation_0-logloss:0.03425	validation_1-logloss:0.25933
    [88]	validation_0-logloss:0.03361	validation_1-logloss:0.25932
    [89]	validation_0-logloss:0.03311	validation_1-logloss:0.26002
    [90]	validation_0-logloss:0.03260	validation_1-logloss:0.25936
    [91]	validation_0-logloss:0.03202	validation_1-logloss:0.25886
    [92]	validation_0-logloss:0.03152	validation_1-logloss:0.25918
    [93]	validation_0-logloss:0.03107	validation_1-logloss:0.25865
    [94]	validation_0-logloss:0.03049	validation_1-logloss:0.25951
    [95]	validation_0-logloss:0.03007	validation_1-logloss:0.26091
    [96]	validation_0-logloss:0.02963	validation_1-logloss:0.26014
    [97]	validation_0-logloss:0.02913	validation_1-logloss:0.25974
    [98]	validation_0-logloss:0.02866	validation_1-logloss:0.25937
    [99]	validation_0-logloss:0.02829	validation_1-logloss:0.25893
    [100]	validation_0-logloss:0.02789	validation_1-logloss:0.25928
    [101]	validation_0-logloss:0.02751	validation_1-logloss:0.25955
    [102]	validation_0-logloss:0.02714	validation_1-logloss:0.25901
    [103]	validation_0-logloss:0.02668	validation_1-logloss:0.25991
    [104]	validation_0-logloss:0.02634	validation_1-logloss:0.25950
    [105]	validation_0-logloss:0.02594	validation_1-logloss:0.25924
    [106]	validation_0-logloss:0.02556	validation_1-logloss:0.25901
    [107]	validation_0-logloss:0.02522	validation_1-logloss:0.25738
    [108]	validation_0-logloss:0.02492	validation_1-logloss:0.25702
    [109]	validation_0-logloss:0.02453	validation_1-logloss:0.25789
    [110]	validation_0-logloss:0.02418	validation_1-logloss:0.25770
    [111]	validation_0-logloss:0.02384	validation_1-logloss:0.25842
    [112]	validation_0-logloss:0.02356	validation_1-logloss:0.25810
    [113]	validation_0-logloss:0.02322	validation_1-logloss:0.25848
    [114]	validation_0-logloss:0.02290	validation_1-logloss:0.25833
    [115]	validation_0-logloss:0.02260	validation_1-logloss:0.25820
    [116]	validation_0-logloss:0.02229	validation_1-logloss:0.25905
    [117]	validation_0-logloss:0.02204	validation_1-logloss:0.25878
    [118]	validation_0-logloss:0.02176	validation_1-logloss:0.25728
    [119]	validation_0-logloss:0.02149	validation_1-logloss:0.25722
    [120]	validation_0-logloss:0.02119	validation_1-logloss:0.25764
    [121]	validation_0-logloss:0.02095	validation_1-logloss:0.25761
    [122]	validation_0-logloss:0.02067	validation_1-logloss:0.25832
    [123]	validation_0-logloss:0.02045	validation_1-logloss:0.25808
    [124]	validation_0-logloss:0.02023	validation_1-logloss:0.25855
    [125]	validation_0-logloss:0.01998	validation_1-logloss:0.25714
    [126]	validation_0-logloss:0.01973	validation_1-logloss:0.25587
    [127]	validation_0-logloss:0.01946	validation_1-logloss:0.25640
    [128]	validation_0-logloss:0.01927	validation_1-logloss:0.25685
    [129]	validation_0-logloss:0.01908	validation_1-logloss:0.25665
    [130]	validation_0-logloss:0.01886	validation_1-logloss:0.25712
    [131]	validation_0-logloss:0.01863	validation_1-logloss:0.25609
    [132]	validation_0-logloss:0.01839	validation_1-logloss:0.25649
    [133]	validation_0-logloss:0.01816	validation_1-logloss:0.25789
    [134]	validation_0-logloss:0.01802	validation_1-logloss:0.25811
    [135]	validation_0-logloss:0.01785	validation_1-logloss:0.25794
    [136]	validation_0-logloss:0.01763	validation_1-logloss:0.25876
    [137]	validation_0-logloss:0.01748	validation_1-logloss:0.25884
    [138]	validation_0-logloss:0.01732	validation_1-logloss:0.25867
    [139]	validation_0-logloss:0.01719	validation_1-logloss:0.25876
    [140]	validation_0-logloss:0.01696	validation_1-logloss:0.25987
    [141]	validation_0-logloss:0.01681	validation_1-logloss:0.25960
    [142]	validation_0-logloss:0.01669	validation_1-logloss:0.25982
    [143]	validation_0-logloss:0.01656	validation_1-logloss:0.25992
    [144]	validation_0-logloss:0.01638	validation_1-logloss:0.26035
    [145]	validation_0-logloss:0.01623	validation_1-logloss:0.26055
    [146]	validation_0-logloss:0.01606	validation_1-logloss:0.26092
    [147]	validation_0-logloss:0.01589	validation_1-logloss:0.26137
    [148]	validation_0-logloss:0.01572	validation_1-logloss:0.25999
    [149]	validation_0-logloss:0.01557	validation_1-logloss:0.26028
    [150]	validation_0-logloss:0.01546	validation_1-logloss:0.26048
    [151]	validation_0-logloss:0.01531	validation_1-logloss:0.26142
    [152]	validation_0-logloss:0.01515	validation_1-logloss:0.26188
    [153]	validation_0-logloss:0.01501	validation_1-logloss:0.26227
    [154]	validation_0-logloss:0.01486	validation_1-logloss:0.26287
    [155]	validation_0-logloss:0.01476	validation_1-logloss:0.26299
    [156]	validation_0-logloss:0.01461	validation_1-logloss:0.26346
    [157]	validation_0-logloss:0.01448	validation_1-logloss:0.26379
    [158]	validation_0-logloss:0.01434	validation_1-logloss:0.26306
    [159]	validation_0-logloss:0.01424	validation_1-logloss:0.26237
    [160]	validation_0-logloss:0.01410	validation_1-logloss:0.26251
    [161]	validation_0-logloss:0.01401	validation_1-logloss:0.26265
    [162]	validation_0-logloss:0.01392	validation_1-logloss:0.26264
    [163]	validation_0-logloss:0.01380	validation_1-logloss:0.26250
    [164]	validation_0-logloss:0.01372	validation_1-logloss:0.26264
    [165]	validation_0-logloss:0.01359	validation_1-logloss:0.26255
    [166]	validation_0-logloss:0.01350	validation_1-logloss:0.26188
    [167]	validation_0-logloss:0.01342	validation_1-logloss:0.26203
    [168]	validation_0-logloss:0.01331	validation_1-logloss:0.26190
    [169]	validation_0-logloss:0.01319	validation_1-logloss:0.26184
    [170]	validation_0-logloss:0.01312	validation_1-logloss:0.26133
    [171]	validation_0-logloss:0.01304	validation_1-logloss:0.26148
    [172]	validation_0-logloss:0.01297	validation_1-logloss:0.26157
    [173]	validation_0-logloss:0.01285	validation_1-logloss:0.26253
    [174]	validation_0-logloss:0.01278	validation_1-logloss:0.26229
    [175]	validation_0-logloss:0.01267	validation_1-logloss:0.26086
    


```python
get_clf_eval(y_test , ws50_preds, ws50_pred_proba)
```

    오차 행렬
    [[34  3]
     [ 2 75]]
    정확도: 0.9561, 정밀도: 0.9615, 재현율: 0.9740,    F1: 0.9677, AUC:0.9933
    

**early stopping을 10으로 설정하고 재 학습/예측/평가**


```python
# early_stopping_rounds를 10으로 설정하고 재 학습. 
xgb_wrapper.fit(X_tr, y_tr, early_stopping_rounds=10, 
                eval_metric="logloss", eval_set=evals,verbose=True)

ws10_preds = xgb_wrapper.predict(X_test)
ws10_pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1]
get_clf_eval(y_test , ws10_preds, ws10_pred_proba)
```

    [0]	validation_0-logloss:0.65016	validation_1-logloss:0.66183
    [1]	validation_0-logloss:0.61131	validation_1-logloss:0.63609
    [2]	validation_0-logloss:0.57563	validation_1-logloss:0.61144
    [3]	validation_0-logloss:0.54310	validation_1-logloss:0.59204
    [4]	validation_0-logloss:0.51323	validation_1-logloss:0.57329
    [5]	validation_0-logloss:0.48447	validation_1-logloss:0.55037
    [6]	validation_0-logloss:0.45796	validation_1-logloss:0.52929
    [7]	validation_0-logloss:0.43436	validation_1-logloss:0.51534
    [8]	validation_0-logloss:0.41150	validation_1-logloss:0.49718
    [9]	validation_0-logloss:0.39027	validation_1-logloss:0.48154
    [10]	validation_0-logloss:0.37128	validation_1-logloss:0.46990
    [11]	validation_0-logloss:0.35254	validation_1-logloss:0.45474
    [12]	validation_0-logloss:0.33528	validation_1-logloss:0.44229
    [13]	validation_0-logloss:0.31893	validation_1-logloss:0.42961
    [14]	validation_0-logloss:0.30439	validation_1-logloss:0.42065
    [15]	validation_0-logloss:0.29000	validation_1-logloss:0.40958
    [16]	validation_0-logloss:0.27651	validation_1-logloss:0.39887
    [17]	validation_0-logloss:0.26389	validation_1-logloss:0.39050
    [18]	validation_0-logloss:0.25210	validation_1-logloss:0.38254
    [19]	validation_0-logloss:0.24123	validation_1-logloss:0.37393
    [20]	validation_0-logloss:0.23076	validation_1-logloss:0.36789
    [21]	validation_0-logloss:0.22091	validation_1-logloss:0.36017
    [22]	validation_0-logloss:0.21155	validation_1-logloss:0.35421
    [23]	validation_0-logloss:0.20263	validation_1-logloss:0.34683
    [24]	validation_0-logloss:0.19434	validation_1-logloss:0.34111
    [25]	validation_0-logloss:0.18637	validation_1-logloss:0.33634
    [26]	validation_0-logloss:0.17875	validation_1-logloss:0.33082
    [27]	validation_0-logloss:0.17167	validation_1-logloss:0.32675
    [28]	validation_0-logloss:0.16481	validation_1-logloss:0.32099
    [29]	validation_0-logloss:0.15835	validation_1-logloss:0.31671
    [30]	validation_0-logloss:0.15225	validation_1-logloss:0.31277
    [31]	validation_0-logloss:0.14650	validation_1-logloss:0.30882
    [32]	validation_0-logloss:0.14102	validation_1-logloss:0.30437
    [33]	validation_0-logloss:0.13590	validation_1-logloss:0.30103
    [34]	validation_0-logloss:0.13109	validation_1-logloss:0.29794
    [35]	validation_0-logloss:0.12647	validation_1-logloss:0.29499
    [36]	validation_0-logloss:0.12197	validation_1-logloss:0.29295
    [37]	validation_0-logloss:0.11784	validation_1-logloss:0.29043
    [38]	validation_0-logloss:0.11379	validation_1-logloss:0.28927
    [39]	validation_0-logloss:0.10994	validation_1-logloss:0.28578
    [40]	validation_0-logloss:0.10638	validation_1-logloss:0.28364
    [41]	validation_0-logloss:0.10302	validation_1-logloss:0.28183
    [42]	validation_0-logloss:0.09963	validation_1-logloss:0.28005
    [43]	validation_0-logloss:0.09649	validation_1-logloss:0.27972
    [44]	validation_0-logloss:0.09359	validation_1-logloss:0.27744
    [45]	validation_0-logloss:0.09080	validation_1-logloss:0.27542
    [46]	validation_0-logloss:0.08807	validation_1-logloss:0.27504
    [47]	validation_0-logloss:0.08541	validation_1-logloss:0.27458
    [48]	validation_0-logloss:0.08299	validation_1-logloss:0.27348
    [49]	validation_0-logloss:0.08035	validation_1-logloss:0.27247
    [50]	validation_0-logloss:0.07786	validation_1-logloss:0.27163
    [51]	validation_0-logloss:0.07550	validation_1-logloss:0.27094
    [52]	validation_0-logloss:0.07344	validation_1-logloss:0.26967
    [53]	validation_0-logloss:0.07147	validation_1-logloss:0.27008
    [54]	validation_0-logloss:0.06964	validation_1-logloss:0.26890
    [55]	validation_0-logloss:0.06766	validation_1-logloss:0.26854
    [56]	validation_0-logloss:0.06592	validation_1-logloss:0.26900
    [57]	validation_0-logloss:0.06433	validation_1-logloss:0.26790
    [58]	validation_0-logloss:0.06259	validation_1-logloss:0.26663
    [59]	validation_0-logloss:0.06107	validation_1-logloss:0.26743
    [60]	validation_0-logloss:0.05957	validation_1-logloss:0.26610
    [61]	validation_0-logloss:0.05817	validation_1-logloss:0.26644
    [62]	validation_0-logloss:0.05691	validation_1-logloss:0.26673
    [63]	validation_0-logloss:0.05550	validation_1-logloss:0.26550
    [64]	validation_0-logloss:0.05422	validation_1-logloss:0.26443
    [65]	validation_0-logloss:0.05311	validation_1-logloss:0.26500
    [66]	validation_0-logloss:0.05207	validation_1-logloss:0.26591
    [67]	validation_0-logloss:0.05093	validation_1-logloss:0.26501
    [68]	validation_0-logloss:0.04976	validation_1-logloss:0.26435
    [69]	validation_0-logloss:0.04872	validation_1-logloss:0.26360
    [70]	validation_0-logloss:0.04776	validation_1-logloss:0.26319
    [71]	validation_0-logloss:0.04680	validation_1-logloss:0.26255
    [72]	validation_0-logloss:0.04580	validation_1-logloss:0.26204
    [73]	validation_0-logloss:0.04484	validation_1-logloss:0.26254
    [74]	validation_0-logloss:0.04388	validation_1-logloss:0.26289
    [75]	validation_0-logloss:0.04309	validation_1-logloss:0.26249
    [76]	validation_0-logloss:0.04224	validation_1-logloss:0.26217
    [77]	validation_0-logloss:0.04133	validation_1-logloss:0.26166
    [78]	validation_0-logloss:0.04050	validation_1-logloss:0.26179
    [79]	validation_0-logloss:0.03967	validation_1-logloss:0.26103
    [80]	validation_0-logloss:0.03877	validation_1-logloss:0.26094
    [81]	validation_0-logloss:0.03806	validation_1-logloss:0.26148
    [82]	validation_0-logloss:0.03740	validation_1-logloss:0.26054
    [83]	validation_0-logloss:0.03676	validation_1-logloss:0.25967
    [84]	validation_0-logloss:0.03605	validation_1-logloss:0.25905
    [85]	validation_0-logloss:0.03545	validation_1-logloss:0.26007
    [86]	validation_0-logloss:0.03488	validation_1-logloss:0.25984
    [87]	validation_0-logloss:0.03425	validation_1-logloss:0.25933
    [88]	validation_0-logloss:0.03361	validation_1-logloss:0.25932
    [89]	validation_0-logloss:0.03311	validation_1-logloss:0.26002
    [90]	validation_0-logloss:0.03260	validation_1-logloss:0.25936
    [91]	validation_0-logloss:0.03202	validation_1-logloss:0.25886
    [92]	validation_0-logloss:0.03152	validation_1-logloss:0.25918
    [93]	validation_0-logloss:0.03107	validation_1-logloss:0.25865
    [94]	validation_0-logloss:0.03049	validation_1-logloss:0.25951
    [95]	validation_0-logloss:0.03007	validation_1-logloss:0.26091
    [96]	validation_0-logloss:0.02963	validation_1-logloss:0.26014
    [97]	validation_0-logloss:0.02913	validation_1-logloss:0.25974
    [98]	validation_0-logloss:0.02866	validation_1-logloss:0.25937
    [99]	validation_0-logloss:0.02829	validation_1-logloss:0.25893
    [100]	validation_0-logloss:0.02789	validation_1-logloss:0.25928
    [101]	validation_0-logloss:0.02751	validation_1-logloss:0.25955
    [102]	validation_0-logloss:0.02714	validation_1-logloss:0.25901
    오차 행렬
    [[34  3]
     [ 3 74]]
    정확도: 0.9474, 정밀도: 0.9610, 재현율: 0.9610,    F1: 0.9610, AUC:0.9933
    


```python
from xgboost import plot_importance
import matplotlib.pyplot as plt
%matplotlib inline

fig, ax = plt.subplots(figsize=(10, 12))
# 사이킷런 래퍼 클래스를 입력해도 무방. 
plot_importance(xgb_wrapper, ax=ax)
```




    <AxesSubplot:title={'center':'Feature importance'}, xlabel='F score', ylabel='Features'>




    
![png](output_31_1.png)
    



```python

```
