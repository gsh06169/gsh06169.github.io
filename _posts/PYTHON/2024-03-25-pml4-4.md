---
title: "[Python 머신러닝] 04-4 랜덤 포레스트"

categories: 
  - PYTHON
tags:
  - [Python, 프로그래밍, 머신러닝, 공부]

toc: true
toc_sticky: true
---

# 분류


## 랜덤 포레스트


### 랜덤 포레스트의 개요 및 실습

#### 배깅(Bagging) - 랜덤 포레스(Random Forest)
- 배깅의 대표적인 알고리즘은 랜덤 포레스트이다.
- 랜덤 포레스트는 다재 다능한 알고리즘이다. <br> 앙상블 알고리즘 중 비교적 빠른 수행 속도를 가지고 있으며, 다양한 영역에서 높은 예측 성능을 보이고 있다.

- 랜덤 포레스트는 여러 개의 결정 트리 분류기가 전체 데이터에서 배깅 방식으로 각자의 데이터를 샘플링해 개별적으로 학습을 수행한 뒤 최종적으로 모든 분류기가 보팅을 통해 예측 결정을 하게 된다.


#### 랜덤 포레스트의 부트스트래핑 분할

- 랜덤 포르세트는 개별적인 분류기의 기반 알고리즘은 결정 트리이지만 개별 트리가 학습하는 데이터 세트는 전체 데이터에서 일부가 중첩되게 샘플링된 데이터 세트이다. <br> 이렇게 여러 개의 데이터 세트를 중첩되게 분리하는 것을 부트스트래핑(bootstrapping) 분할 방식이라고 한다. (Bagging이 bootstrap aggregating의 줄임말)
- 원본 데이터의 건수가 10개인 학습 데이터 세트에 랜덤 포레스트를 3개의 결정 트리 기반으로 학습하려고 n_estimators=3으로 하이퍼 파라미터를 부여하면 다음과 같이 데이터 서브세트가 만들어진다.  
  - [ 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 ]  
    ---부트스트래핑 분할---  
  -> 서브세트 #1: [ 1 | 2 | 3 | 3 | 3 | 5 | 6 | 8 | 8 | 9 ]  
  -> 서브세트 #2: [ 1 | 3 | 4 | 5 | 6 | 8 | 8 | 9 | 9 | 10 ]  
  -> 서브세트 #3: [ 1 | 1 | 3 | 4 | 4 | 5 | 6 | 6 | 9 | 9 ]  

### 랜덤 포레스트 하이퍼 파라미터 및 튜닝

사이킷런은 랜덤 포레스트 분류를 위해 RandomForestClassifier 클래스를 제공한다.

`RandomForestClassifier 하이퍼 파라미터`

- n_estimators는 랜덤 포레스트에서 결정 트리의 개수를 지정한다. <br> 디폴트는 100개이다. <br> 많이 설정할 수록 좋은 성능을 기대할 수 있지만 계속 증가시킨다고 성능이 무조건 향상되는 것은 아니다. <br> 또한 늘릴수록 학습 수행 시간이 오래 걸리는 것도 감안해야 한다.
- max_features는 결정 트리에 사용된 max_features 파라미터와 같다. <br> 하지만 RandomForestClassifier의 기본 max_features는 'None'이 아니라 'auto', 즉 'sqrt'와 같다. <br> 따라서 랜덤 포레스트의 트리를 분할하는 피처를 참조할 때 전체 피처가 아니라 sqrt(전체 피처 개수) 만큼 참조한다. (전체 피처가 16개라면 분할을 위해 4개 참조)
- max_depth나 min_samples_leaf와 같이 결정 트리에서 과적합을 개선하기 위해 사용되는 파라미터가 랜덤 포레스트에도 똑같이 적용될 수 있다.


#### <실습>

```python
import pandas as pd

def get_new_feature_name_df(old_feature_name_df):
    feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby('column_name').cumcount(),
                                  columns=['dup_cnt'])
    feature_dup_df = feature_dup_df.reset_index()
    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')
    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(lambda x : x[0]+'_'+str(x[1]) 
                                                                                         if x[1] >0 else x[0] ,  axis=1)
    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)
    return new_feature_name_df

def get_human_dataset( ):
    
    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.
    feature_name_df = pd.read_csv('./human_activity/features.txt',sep='\s+',
                        header=None,names=['column_index','column_name'])
    
    # 중복된 피처명을 수정하는 get_new_feature_name_df()를 이용, 신규 피처명 DataFrame생성. 
    new_feature_name_df = get_new_feature_name_df(feature_name_df)
    
    # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환
    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()
    
    # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용
    X_train = pd.read_csv('./human_activity/train/X_train.txt',sep='\s+', names=feature_name )
    X_test = pd.read_csv('./human_activity/test/X_test.txt',sep='\s+', names=feature_name)
    
    # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여
    y_train = pd.read_csv('./human_activity/train/y_train.txt',sep='\s+',header=None,names=['action'])
    y_test = pd.read_csv('./human_activity/test/y_test.txt',sep='\s+',header=None,names=['action'])
    
    # 로드된 학습/테스트용 DataFrame을 모두 반환 
    return X_train, X_test, y_train, y_test
```


```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

# 결정 트리에서 사용한 get_human_dataset( )을 이용해 학습/테스트용 DataFrame 반환
X_train, X_test, y_train, y_test = get_human_dataset()

# 랜덤 포레스트 학습 및 별도의 테스트 셋으로 예측 성능 평가
rf_clf = RandomForestClassifier(n_estimators=100, random_state=0, max_depth=8)
rf_clf.fit(X_train , y_train)
pred = rf_clf.predict(X_test)
accuracy = accuracy_score(y_test , pred)
print('랜덤 포레스트 정확도: {0:.4f}'.format(accuracy))
```

    랜덤 포레스트 정확도: 0.9196
    


```python
from sklearn.model_selection import GridSearchCV

params = {
    'max_depth': [8, 16, 24],
    'min_samples_leaf' : [1, 6, 12],
    'min_samples_split' : [2, 8, 16]
}
# RandomForestClassifier 객체 생성 후 GridSearchCV 수행
rf_clf = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)
grid_cv = GridSearchCV(rf_clf , param_grid=params , cv=2, n_jobs=-1 )
grid_cv.fit(X_train , y_train)

print('최적 하이퍼 파라미터:\n', grid_cv.best_params_)
print('최고 예측 정확도: {0:.4f}'.format(grid_cv.best_score_))
```

    최적 하이퍼 파라미터:
     {'max_depth': 16, 'min_samples_leaf': 6, 'min_samples_split': 2}
    최고 예측 정확도: 0.9165
    


```python
rf_clf1 = RandomForestClassifier(n_estimators=100,  min_samples_leaf=6, max_depth=16,
                                 min_samples_split=2, random_state=0)
rf_clf1.fit(X_train , y_train)
pred = rf_clf1.predict(X_test)
print('예측 정확도: {0:.4f}'.format(accuracy_score(y_test , pred)))
```

    예측 정확도: 0.9260
    


```python
ftr_importances_values = rf_clf1.feature_importances_
ftr_importances = pd.Series(ftr_importances_values,index=X_train.columns)
ftr_importances.sort_values(ascending=False)[:20]
```




    angle(X,gravityMean)               0.035312
    tGravityAcc-mean()-Y               0.033729
    tGravityAcc-min()-X                0.030276
    angle(Y,gravityMean)               0.029013
    tGravityAcc-max()-X                0.027538
    tGravityAcc-mean()-X               0.024708
    tGravityAcc-energy()-X             0.024389
    tGravityAcc-min()-Y                0.023339
    tGravityAcc-max()-Y                0.021011
    fBodyAccJerk-bandsEnergy()-1,8     0.018636
    tGravityAcc-energy()-Y             0.015461
    tBodyAcc-max()-X                   0.013750
    tBodyAccJerk-entropy()-X           0.013433
    tGravityAcc-arCoeff()-Z,1          0.013150
    tGravityAcc-max()-Z                0.013125
    fBodyAccMag-energy()               0.012262
    tGravityAcc-energy()-Z             0.010683
    fBodyAccJerk-bandsEnergy()-1,16    0.010095
    fBodyAcc-mad()-X                   0.010036
    angle(Z,gravityMean)               0.009483
    dtype: float64




```python
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

ftr_importances_values = rf_clf1.feature_importances_
ftr_importances = pd.Series(ftr_importances_values,index=X_train.columns  )
ftr_top20 = ftr_importances.sort_values(ascending=False)[:20]

plt.figure(figsize=(8,6))
plt.title('Feature importances Top 20')
sns.barplot(x=ftr_top20 , y = ftr_top20.index)
plt.show()
```


    
![output_10_1](https://github.com/gsh06169/gsh06169/assets/150469460/d9dbc113-ea50-457d-8366-519f37fde37f)