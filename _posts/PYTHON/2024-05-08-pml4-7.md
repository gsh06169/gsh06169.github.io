---
title: "[Python 머신러닝] 04-7 LightGBM"

categories: 
  - PYTHON
tags:
  - [Python, 프로그래밍, 머신러닝, 공부]

toc: true
toc_sticky: true
---

# 분류

## LightGBM

### LightGBM 개요

XGBoost 대비 장점  
    - 더 빠른 학습과 예측 수행 시간  
    - 더 작은 메모리 사용량  
    - 카테고리형 피처의 자동 변환과 최적 분할 (원-핫 인코딩 등을 사용하지 않고도 카테고리형 피처를 최적으로 변환하고 이에 따른 노드 분할 수행)

### LightGBM 트리 분할 방식 - 리프 중심
![IMG_2555](https://github.com/gsh06169/gsh06169/assets/150469460/be003f79-edf8-479d-97c4-77c0f6547e91)

LightGBM은 일반 GBM 계열의 트리 분할 방법과 다르게 리프 중심 트리 분할(Leaf Wise) 방식을 사용한다.

```python
import lightgbm

print(lightgbm.__version__)
```

    3.3.2
    

### LightGBM 적용 – 위스콘신 Breast Cancer Prediction


```python
# LightGBM의 파이썬 패키지인 lightgbm에서 LGBMClassifier 임포트
from lightgbm import LGBMClassifier

import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

dataset = load_breast_cancer()

cancer_df = pd.DataFrame(data=dataset.data, columns=dataset.feature_names)
cancer_df['target']= dataset.target

cancer_df.head()

```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean radius</th>
      <th>mean texture</th>
      <th>mean perimeter</th>
      <th>mean area</th>
      <th>mean smoothness</th>
      <th>mean compactness</th>
      <th>mean concavity</th>
      <th>mean concave points</th>
      <th>mean symmetry</th>
      <th>mean fractal dimension</th>
      <th>...</th>
      <th>worst texture</th>
      <th>worst perimeter</th>
      <th>worst area</th>
      <th>worst smoothness</th>
      <th>worst compactness</th>
      <th>worst concavity</th>
      <th>worst concave points</th>
      <th>worst symmetry</th>
      <th>worst fractal dimension</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17.99</td>
      <td>10.38</td>
      <td>122.80</td>
      <td>1001.0</td>
      <td>0.11840</td>
      <td>0.27760</td>
      <td>0.3001</td>
      <td>0.14710</td>
      <td>0.2419</td>
      <td>0.07871</td>
      <td>...</td>
      <td>17.33</td>
      <td>184.60</td>
      <td>2019.0</td>
      <td>0.1622</td>
      <td>0.6656</td>
      <td>0.7119</td>
      <td>0.2654</td>
      <td>0.4601</td>
      <td>0.11890</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>20.57</td>
      <td>17.77</td>
      <td>132.90</td>
      <td>1326.0</td>
      <td>0.08474</td>
      <td>0.07864</td>
      <td>0.0869</td>
      <td>0.07017</td>
      <td>0.1812</td>
      <td>0.05667</td>
      <td>...</td>
      <td>23.41</td>
      <td>158.80</td>
      <td>1956.0</td>
      <td>0.1238</td>
      <td>0.1866</td>
      <td>0.2416</td>
      <td>0.1860</td>
      <td>0.2750</td>
      <td>0.08902</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>19.69</td>
      <td>21.25</td>
      <td>130.00</td>
      <td>1203.0</td>
      <td>0.10960</td>
      <td>0.15990</td>
      <td>0.1974</td>
      <td>0.12790</td>
      <td>0.2069</td>
      <td>0.05999</td>
      <td>...</td>
      <td>25.53</td>
      <td>152.50</td>
      <td>1709.0</td>
      <td>0.1444</td>
      <td>0.4245</td>
      <td>0.4504</td>
      <td>0.2430</td>
      <td>0.3613</td>
      <td>0.08758</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>11.42</td>
      <td>20.38</td>
      <td>77.58</td>
      <td>386.1</td>
      <td>0.14250</td>
      <td>0.28390</td>
      <td>0.2414</td>
      <td>0.10520</td>
      <td>0.2597</td>
      <td>0.09744</td>
      <td>...</td>
      <td>26.50</td>
      <td>98.87</td>
      <td>567.7</td>
      <td>0.2098</td>
      <td>0.8663</td>
      <td>0.6869</td>
      <td>0.2575</td>
      <td>0.6638</td>
      <td>0.17300</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>20.29</td>
      <td>14.34</td>
      <td>135.10</td>
      <td>1297.0</td>
      <td>0.10030</td>
      <td>0.13280</td>
      <td>0.1980</td>
      <td>0.10430</td>
      <td>0.1809</td>
      <td>0.05883</td>
      <td>...</td>
      <td>16.67</td>
      <td>152.20</td>
      <td>1575.0</td>
      <td>0.1374</td>
      <td>0.2050</td>
      <td>0.4000</td>
      <td>0.1625</td>
      <td>0.2364</td>
      <td>0.07678</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 31 columns</p>
</div>




```python

```


```python
X_features = cancer_df.iloc[:, :-1]
y_label = cancer_df.iloc[:, -1]

# 전체 데이터 중 80%는 학습용 데이터, 20%는 테스트용 데이터 추출
X_train, X_test, y_train, y_test=train_test_split(X_features, y_label,
                                         test_size=0.2, random_state=156 )

# 위에서 만든 X_train, y_train을 다시 쪼개서 90%는 학습과 10%는 검증용 데이터로 분리  
X_tr, X_val, y_tr, y_val= train_test_split(X_train, y_train,
                                         test_size=0.1, random_state=156 )

# 앞서 XGBoost와 동일하게 n_estimators는 400 설정. 
lgbm_wrapper = LGBMClassifier(n_estimators=400, learning_rate=0.05)

# LightGBM도 XGBoost와 동일하게 조기 중단 수행 가능. 
evals = [(X_tr, y_tr), (X_val, y_val)]
lgbm_wrapper.fit(X_tr, y_tr, early_stopping_rounds=50, eval_metric="logloss", 
                 eval_set=evals, verbose=True)
preds = lgbm_wrapper.predict(X_test)
pred_proba = lgbm_wrapper.predict_proba(X_test)[:, 1]
```

    [1]	training's binary_logloss: 0.625671	valid_1's binary_logloss: 0.628248
    [2]	training's binary_logloss: 0.588173	valid_1's binary_logloss: 0.601106
    [3]	training's binary_logloss: 0.554518	valid_1's binary_logloss: 0.577587
    [4]	training's binary_logloss: 0.523972	valid_1's binary_logloss: 0.556324
    [5]	training's binary_logloss: 0.49615	valid_1's binary_logloss: 0.537407
    [6]	training's binary_logloss: 0.470108	valid_1's binary_logloss: 0.519401
    [7]	training's binary_logloss: 0.446647	valid_1's binary_logloss: 0.502637
    [8]	training's binary_logloss: 0.425055	valid_1's binary_logloss: 0.488311
    [9]	training's binary_logloss: 0.405125	valid_1's binary_logloss: 0.474664
    [10]	training's binary_logloss: 0.386526	valid_1's binary_logloss: 0.461267
    [11]	training's binary_logloss: 0.367027	valid_1's binary_logloss: 0.444274
    [12]	training's binary_logloss: 0.350713	valid_1's binary_logloss: 0.432755
    [13]	training's binary_logloss: 0.334601	valid_1's binary_logloss: 0.421371
    [14]	training's binary_logloss: 0.319854	valid_1's binary_logloss: 0.411418
    [15]	training's binary_logloss: 0.306374	valid_1's binary_logloss: 0.402989
    [16]	training's binary_logloss: 0.293116	valid_1's binary_logloss: 0.393973
    [17]	training's binary_logloss: 0.280812	valid_1's binary_logloss: 0.384801
    [18]	training's binary_logloss: 0.268352	valid_1's binary_logloss: 0.376191
    [19]	training's binary_logloss: 0.256942	valid_1's binary_logloss: 0.368378
    [20]	training's binary_logloss: 0.246443	valid_1's binary_logloss: 0.362062
    [21]	training's binary_logloss: 0.236874	valid_1's binary_logloss: 0.355162
    [22]	training's binary_logloss: 0.227501	valid_1's binary_logloss: 0.348933
    [23]	training's binary_logloss: 0.218988	valid_1's binary_logloss: 0.342819
    [24]	training's binary_logloss: 0.210621	valid_1's binary_logloss: 0.337386
    [25]	training's binary_logloss: 0.202076	valid_1's binary_logloss: 0.331523
    [26]	training's binary_logloss: 0.194199	valid_1's binary_logloss: 0.326349
    [27]	training's binary_logloss: 0.187107	valid_1's binary_logloss: 0.322785
    [28]	training's binary_logloss: 0.180535	valid_1's binary_logloss: 0.317877
    [29]	training's binary_logloss: 0.173834	valid_1's binary_logloss: 0.313928
    [30]	training's binary_logloss: 0.167198	valid_1's binary_logloss: 0.310105
    [31]	training's binary_logloss: 0.161229	valid_1's binary_logloss: 0.307107
    [32]	training's binary_logloss: 0.155494	valid_1's binary_logloss: 0.303837
    [33]	training's binary_logloss: 0.149125	valid_1's binary_logloss: 0.300315
    [34]	training's binary_logloss: 0.144045	valid_1's binary_logloss: 0.297816
    [35]	training's binary_logloss: 0.139341	valid_1's binary_logloss: 0.295387
    [36]	training's binary_logloss: 0.134625	valid_1's binary_logloss: 0.293063
    [37]	training's binary_logloss: 0.129167	valid_1's binary_logloss: 0.289127
    [38]	training's binary_logloss: 0.12472	valid_1's binary_logloss: 0.288697
    [39]	training's binary_logloss: 0.11974	valid_1's binary_logloss: 0.28576
    [40]	training's binary_logloss: 0.115054	valid_1's binary_logloss: 0.282853
    [41]	training's binary_logloss: 0.110662	valid_1's binary_logloss: 0.279441
    [42]	training's binary_logloss: 0.106358	valid_1's binary_logloss: 0.28113
    [43]	training's binary_logloss: 0.102324	valid_1's binary_logloss: 0.279139
    [44]	training's binary_logloss: 0.0985699	valid_1's binary_logloss: 0.276465
    [45]	training's binary_logloss: 0.094858	valid_1's binary_logloss: 0.275946
    [46]	training's binary_logloss: 0.0912486	valid_1's binary_logloss: 0.272819
    [47]	training's binary_logloss: 0.0883115	valid_1's binary_logloss: 0.272306
    [48]	training's binary_logloss: 0.0849963	valid_1's binary_logloss: 0.270452
    [49]	training's binary_logloss: 0.0821742	valid_1's binary_logloss: 0.268671
    [50]	training's binary_logloss: 0.0789991	valid_1's binary_logloss: 0.267587
    [51]	training's binary_logloss: 0.0761072	valid_1's binary_logloss: 0.26626
    [52]	training's binary_logloss: 0.0732567	valid_1's binary_logloss: 0.265542
    [53]	training's binary_logloss: 0.0706388	valid_1's binary_logloss: 0.264547
    [54]	training's binary_logloss: 0.0683911	valid_1's binary_logloss: 0.26502
    [55]	training's binary_logloss: 0.0659347	valid_1's binary_logloss: 0.264388
    [56]	training's binary_logloss: 0.0636873	valid_1's binary_logloss: 0.263128
    [57]	training's binary_logloss: 0.0613354	valid_1's binary_logloss: 0.26231
    [58]	training's binary_logloss: 0.0591944	valid_1's binary_logloss: 0.262011
    [59]	training's binary_logloss: 0.057033	valid_1's binary_logloss: 0.261454
    [60]	training's binary_logloss: 0.0550801	valid_1's binary_logloss: 0.260746
    [61]	training's binary_logloss: 0.0532381	valid_1's binary_logloss: 0.260236
    [62]	training's binary_logloss: 0.0514074	valid_1's binary_logloss: 0.261586
    [63]	training's binary_logloss: 0.0494837	valid_1's binary_logloss: 0.261797
    [64]	training's binary_logloss: 0.0477826	valid_1's binary_logloss: 0.262533
    [65]	training's binary_logloss: 0.0460364	valid_1's binary_logloss: 0.263305
    [66]	training's binary_logloss: 0.0444552	valid_1's binary_logloss: 0.264072
    [67]	training's binary_logloss: 0.0427638	valid_1's binary_logloss: 0.266223
    [68]	training's binary_logloss: 0.0412449	valid_1's binary_logloss: 0.266817
    [69]	training's binary_logloss: 0.0398589	valid_1's binary_logloss: 0.267819
    [70]	training's binary_logloss: 0.0383095	valid_1's binary_logloss: 0.267484
    [71]	training's binary_logloss: 0.0368803	valid_1's binary_logloss: 0.270233
    [72]	training's binary_logloss: 0.0355637	valid_1's binary_logloss: 0.268442
    [73]	training's binary_logloss: 0.0341747	valid_1's binary_logloss: 0.26895
    [74]	training's binary_logloss: 0.0328302	valid_1's binary_logloss: 0.266958
    [75]	training's binary_logloss: 0.0317853	valid_1's binary_logloss: 0.268091
    [76]	training's binary_logloss: 0.0305626	valid_1's binary_logloss: 0.266419
    [77]	training's binary_logloss: 0.0295001	valid_1's binary_logloss: 0.268588
    [78]	training's binary_logloss: 0.0284699	valid_1's binary_logloss: 0.270964
    [79]	training's binary_logloss: 0.0273953	valid_1's binary_logloss: 0.270293
    [80]	training's binary_logloss: 0.0264668	valid_1's binary_logloss: 0.270523
    [81]	training's binary_logloss: 0.0254636	valid_1's binary_logloss: 0.270683
    [82]	training's binary_logloss: 0.0245911	valid_1's binary_logloss: 0.273187
    [83]	training's binary_logloss: 0.0236486	valid_1's binary_logloss: 0.275994
    [84]	training's binary_logloss: 0.0228047	valid_1's binary_logloss: 0.274053
    [85]	training's binary_logloss: 0.0221693	valid_1's binary_logloss: 0.273211
    [86]	training's binary_logloss: 0.0213043	valid_1's binary_logloss: 0.272626
    [87]	training's binary_logloss: 0.0203934	valid_1's binary_logloss: 0.27534
    [88]	training's binary_logloss: 0.0195552	valid_1's binary_logloss: 0.276228
    [89]	training's binary_logloss: 0.0188623	valid_1's binary_logloss: 0.27525
    [90]	training's binary_logloss: 0.0183664	valid_1's binary_logloss: 0.276485
    [91]	training's binary_logloss: 0.0176788	valid_1's binary_logloss: 0.277052
    [92]	training's binary_logloss: 0.0170059	valid_1's binary_logloss: 0.277686
    [93]	training's binary_logloss: 0.0164317	valid_1's binary_logloss: 0.275332
    [94]	training's binary_logloss: 0.015878	valid_1's binary_logloss: 0.276236
    [95]	training's binary_logloss: 0.0152959	valid_1's binary_logloss: 0.274538
    [96]	training's binary_logloss: 0.0147216	valid_1's binary_logloss: 0.275244
    [97]	training's binary_logloss: 0.0141758	valid_1's binary_logloss: 0.275829
    [98]	training's binary_logloss: 0.0136551	valid_1's binary_logloss: 0.276654
    [99]	training's binary_logloss: 0.0131585	valid_1's binary_logloss: 0.277859
    [100]	training's binary_logloss: 0.0126961	valid_1's binary_logloss: 0.279265
    [101]	training's binary_logloss: 0.0122421	valid_1's binary_logloss: 0.276695
    [102]	training's binary_logloss: 0.0118067	valid_1's binary_logloss: 0.278488
    [103]	training's binary_logloss: 0.0113994	valid_1's binary_logloss: 0.278932
    [104]	training's binary_logloss: 0.0109799	valid_1's binary_logloss: 0.280997
    [105]	training's binary_logloss: 0.0105953	valid_1's binary_logloss: 0.281454
    [106]	training's binary_logloss: 0.0102381	valid_1's binary_logloss: 0.282058
    [107]	training's binary_logloss: 0.00986714	valid_1's binary_logloss: 0.279275
    [108]	training's binary_logloss: 0.00950998	valid_1's binary_logloss: 0.281427
    [109]	training's binary_logloss: 0.00915965	valid_1's binary_logloss: 0.280752
    [110]	training's binary_logloss: 0.00882581	valid_1's binary_logloss: 0.282152
    [111]	training's binary_logloss: 0.00850714	valid_1's binary_logloss: 0.280894
    


```python
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import f1_score, roc_auc_score

def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix( y_test, pred)
    accuracy = accuracy_score(y_test , pred)
    precision = precision_score(y_test , pred)
    recall = recall_score(y_test , pred)
    f1 = f1_score(y_test,pred)
    # ROC-AUC 추가 
    roc_auc = roc_auc_score(y_test, pred_proba)
    print('오차 행렬')
    print(confusion)
    # ROC-AUC print 추가
    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\
    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))
```


```python
get_clf_eval(y_test, preds, pred_proba)
```

    오차 행렬
    [[34  3]
     [ 2 75]]
    정확도: 0.9561, 정밀도: 0.9615, 재현율: 0.9740,    F1: 0.9677, AUC:0.9877
    


```python
# plot_importance( )를 이용하여 feature 중요도 시각화
from lightgbm import plot_importance
import matplotlib.pyplot as plt
%matplotlib inline

fig, ax = plt.subplots(figsize=(10, 12))
plot_importance(lgbm_wrapper, ax=ax)
plt.show()
```


    
![png](output_7_0.png)
    



```python

```
