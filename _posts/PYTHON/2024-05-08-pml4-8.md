---
title: "[Python 머신러닝] 04-8 베이지안 최적화 기반의 HyperOpt를 이용한 하이퍼 파라미터 튜닝"

categories: 
  - PYTHON
tags:
  - [Python, 프로그래밍, 머신러닝, 공부]

toc: true
toc_sticky: true
---

# 분류

## 베이지안 최적화 기반의 HyperOpt를 이용한 하이퍼 파라미터 튜닝

### 하이퍼 파라미터 튜닝 수행 방법

- Grid Search
- Random Search
- Bayesian Optimization
- 수동 튜닝

#### 하이퍼 파라미터 튜닝의 주요 이슈

- Gridient Boosting 기반 알고리즘은 튜닝 해야 할 하이퍼 파라미터 개수가 많고 범위가 넓어서 가능한 개별 경우의 수가 너무 많음
- 이러한 경우의 수가 많을 경우 데이터가 크면 하이퍼 파라미터 튜닝에 굉장히 오랜 시간이 투입되어야 함


#### Grid Search와 Random Search의 주요 이슈

GridSearchCV(classifier, params, cv=3) / RandomizedSearch(classifier, parmas, cv=3, n_iter=10)

```python
params = {  
    'max_depth' = [10, 20, 30, 40, 50],  
    'num_leaves' = [35, 45, 55, 65],  
    'colsample_bytree' = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],      
    'subsample' = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],
    'max_bin' = [100, 200, 300, 400],
    'min_child_weight' = [10, 20, 30, 40]
}
```

- GridSearchCV는 수행 시간이 너무 오래 걸림 <br> 개별 하이퍼 파라미터들을 Grid 형태로 지정하는 것은 한계가 존재 (데이터 세트가 작을 때 유리)
- RandomizedSearch는 수행 시간은 줄여 주지만, Random한 선택으로 최적 하이퍼 파라미터 검출에 태생적 제약 (데이터 세트가 클 때 유리)
- 두 가지 방법 모두 iteration 중에 어느 정도 최적화된 하이퍼 파라미터들을 활용하면서 최적화를 수행할 수 없음


#### Bayesian 최적화가 필요한 순간

- 가능한 최소의 시도로 최적의 답을 찾아야 할 경우
- 개별 시도가 너무 많은 시간/자원이 필요할 때

### 베이지안 최적화 개요

- 베이지안 최적화는 미지의 함수가 반환하는 값의 최소 또는 최대값을 만드는 최적해를 짧은 반복을 통해 찾아내는 최적화 방식
- 베이지안 최적화는 새로운 데이터를 입력 받았을 때 최적 함수를 예측하는 사후 모델을 개선해 나가면서 최적 함수를 도출
- 대체 모델(Surrogate Model)과 획득 함수로 구성되며, 대체 모델은 획득 함수로 부터 최적 입력 값을 추천 받은 뒤 이를 기반으로 최적 함수 모델을 개선
- 획득 함수는 개선된 대체 모델을 기반으로 다시 최적 입력 값을 계산

#### 베이지안 최적화 수행 단계

Step 1: 최초에는 랜덤하게 하이퍼 파라미터들을 샘플링하여 성능 결과를 관측

![IMG_2557](https://github.com/gsh06169/gsh06169/assets/150469460/fcecc2f1-b5c7-4e2d-b5a0-35786e9e659e)

Step 2: 관측된 값을 기반으로 대체 모델은 최적 함수를 예측 추정

![IMG_2558](https://github.com/gsh06169/gsh06169/assets/150469460/af7dadd8-2d1b-45da-9213-6eb56244074c)


Step 3: 획득 함수에서 다음으로 관측할 하이퍼 파라미터 추출

![IMG_2559](https://github.com/gsh06169/gsh06169/assets/150469460/aaaf8c21-5124-412a-8b76-dfe3c19d9e2a)

Step 4: 해당 하이퍼 파라미터로 관측된 값을 기반으로 대체 모델은 다시 최적 함수 예측 추정

![IMG_2561](https://github.com/gsh06169/gsh06169/assets/150469460/4cedb77f-ebff-4f87-9495-cb62760c11a3)

#### 베이지안 최적화 구현 요소

1. 입력값 범위 
```python
search_space = {'x': (-10, 10), 'y': (-15, 15)}
```

2. 함수  
```python
def black_box_function(x, y):  
  return -x**2 - 20*y
```

3. 함수 반환 최솟값 유추
```python
fmin(fn=black_box_function, space=search_space,
algo=tpe.suggest, max_evals=20, trials=trial_val)
```

#### 베이지안 최적화를 구현한 주요 패키지
- HyperOpt
- Bayesian optimization
- Optuna



### HyperOpt 사용하기


#### HyperOpt를 통한 최적화 예시

1. Search Sapce (입력값 범위)
```python
search_space = {'x': hp.quniform('x', 5, 15, 1),
                'y': hp.uniform('y', 0.01, 01)}
```

2. 목적 함수
```python
def objective_func(search_space):
  x = search_space['x']
  y = search_space['y']
  print('x:', x, 'y:', y)
  return{'loss': x**2 + y*20, 'status': STATUS_OK}
```

3. 목적 함수 반환 최솟값 유추
```python
best = fmin(fn=objective_func, space=search_space, algo=algo,
            max_evals=5, trials=trials)
```

[output]
```
[{'loss': 81.20833131199375},
{'loss': 169.20757538485393},
{'loss': 121.10536542037384},
{'loss': 64.08021188657003},
{'loss': 81.42067134007004}]
```

#### HyperOpt의 주요 구성 요소

구성 요소 | 설명 
---|---
search_space <br> (입력값 범위) | * 여러 개의 입력 변수들과 이들 값의 범위를 지정 <br> * hp.quniform(label, low, high, q): label로 지정된 입력값 변수 검색 공간을 최솟값 low에서 최댓값 high까지 q의 간격을 가지고 설정 <br> * hp.uniform(label, low, high): 최솟값 low에서 최댓값 high까지 정규 분포 형태의 검색 공간 설정 <br> * hp.randint(label, upper): 0부터 최댓값 upper까지 random한 정수 값으로 검색 공간 설정 <br> * hp.loguniform(label, low, high): exp(uniform(low, high))값을 반환하며, 반환 값의 log 변환 된 값은 정규 분포 형태를 가지는 검색 공간 설정
목적 함수 | * serach space를 입력 받아 로직에 따라 loss값을 계산하고 이를 반환하는 함수 <br> 반드시 dictionary 형태의 값을 반환하고 여기에 'loss': loss값이 기재되어야 함
목적 함수의 최솟값을 찾는 함수 | * 목적 함수를 실행하여 최소 반환값(loss)을 최적으로 찾아 내는 함수 <br> Bayesian 최적화 기법으로 입력 변수들의 search space 상에서 정해진 횟수만큼 입력하여 목적 함수의 반환값(loss)을 최적으로 찾아냄 <br> hyperopt는 일르 위해 fmin( ) 함수를 제공 <br> fmin( ) 함수의 인자로 목적 함수, search space, 베이지안 최적화 기법 유형, 최적화 시도 횟수, 최적화 로그 기록 객체를 인자로 넣어줌 <br> best = fmin(objective, sapce=hp.uniform('x', -10, 10), algo=tpe.suggest, max_evals=100, trials=trials)



#### <실습>


```python
import hyperopt

print(hyperopt.__version__)
```

    0.2.7
    


```python
#!pip install hyperopt==0.2.7
```


```python
from hyperopt import hp

# -10 ~ 10까지 1간격을 가지는 입력 변수 x 집합값과 -15 ~ 15까지 1간격을 가지는 입력 변수  y 집합값 설정.
search_space = {'x': hp.quniform('x', -10, 10, 1),  'y': hp.quniform('y', -15, 15, 1) }
```


```python
search_space
```




    {'x': <hyperopt.pyll.base.Apply at 0x1db7dd926d0>,
     'y': <hyperopt.pyll.base.Apply at 0x1db006dd130>}




```python
from hyperopt import STATUS_OK

# 목적 함수를 생성. 입력 변수값과 입력 변수 검색 범위를 가지는 딕셔너리를 인자로 받고, 특정 값을 반환
def objective_func(search_space):
    x = search_space['x']
    y = search_space['y']
    retval = x**2 - 20*y
    
    return retval # return {'loss': retval, 'status':STATUS_OK}
```


```python
from hyperopt import fmin, tpe, Trials
import numpy as np

# 입력 결괏값을 저장한 Trials 객체값 생성.
trial_val = Trials()

# 목적 함수의 최솟값을 반환하는 최적 입력 변숫값을 5번의 입력값 시도(max_evals=5)로 찾아냄.
best_01 = fmin(fn=objective_func, space=search_space, algo=tpe.suggest, max_evals=5
               , trials=trial_val, rstate=np.random.default_rng(seed=0)
              )
print('best:', best_01)
```

    100%|████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 1109.37trial/s, best loss: -224.0]
    best: {'x': -4.0, 'y': 12.0}
    


```python
trial_val = Trials()

# max_evals를 20회로 늘려서 재테스트
best_02 = fmin(fn=objective_func, space=search_space, algo=tpe.suggest, max_evals=20
               , trials=trial_val, rstate=np.random.default_rng(seed=0))
print('best:', best_02)
```

    100%|██████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 1176.21trial/s, best loss: -296.0]
    best: {'x': 2.0, 'y': 15.0}
    


```python
trial_val
```




    <hyperopt.base.Trials at 0x1db006dd370>



* HyperOpt 수행 시 적용된 입력 값들과 목적 함수 반환값 보기


```python
# fmin( )에 인자로 들어가는 Trials 객체의 result 속성에 파이썬 리스트로 목적 함수 반환값들이 저장됨
# 리스트 내부의 개별 원소는 {'loss':함수 반환값, 'status':반환 상태값} 와 같은 딕셔너리임. 
print(trial_val.results)
```

    [{'loss': -64.0, 'status': 'ok'}, {'loss': -184.0, 'status': 'ok'}, {'loss': 56.0, 'status': 'ok'}, {'loss': -224.0, 'status': 'ok'}, {'loss': 61.0, 'status': 'ok'}, {'loss': -296.0, 'status': 'ok'}, {'loss': -40.0, 'status': 'ok'}, {'loss': 281.0, 'status': 'ok'}, {'loss': 64.0, 'status': 'ok'}, {'loss': 100.0, 'status': 'ok'}, {'loss': 60.0, 'status': 'ok'}, {'loss': -39.0, 'status': 'ok'}, {'loss': 1.0, 'status': 'ok'}, {'loss': -164.0, 'status': 'ok'}, {'loss': 21.0, 'status': 'ok'}, {'loss': -56.0, 'status': 'ok'}, {'loss': 284.0, 'status': 'ok'}, {'loss': 176.0, 'status': 'ok'}, {'loss': -171.0, 'status': 'ok'}, {'loss': 0.0, 'status': 'ok'}]
    


```python
# Trials 객체의 vals 속성에 {'입력변수명':개별 수행 시마다 입력된 값 리스트} 형태로 저장됨.
print(trial_val.vals)
```

    {'x': [-6.0, -4.0, 4.0, -4.0, 9.0, 2.0, 10.0, -9.0, -8.0, -0.0, -0.0, 1.0, 9.0, 6.0, 9.0, 2.0, -2.0, -4.0, 7.0, -0.0], 'y': [5.0, 10.0, -2.0, 12.0, 1.0, 15.0, 7.0, -10.0, 0.0, -5.0, -3.0, 2.0, 4.0, 10.0, 3.0, 3.0, -14.0, -8.0, 11.0, -0.0]}
    


```python
import pandas as pd 

# results에서 loss 키값에 해당하는 밸류들을 추출하여 list로 생성. 
losses = [loss_dict['loss'] for loss_dict in trial_val.results]

# DataFrame으로 생성. 
result_df = pd.DataFrame({'x': trial_val.vals['x'],
                         'y': trial_val.vals['y'],
                          'losses': losses
                         }
                        )
result_df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>x</th>
      <th>y</th>
      <th>losses</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-6.0</td>
      <td>5.0</td>
      <td>-64.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-4.0</td>
      <td>10.0</td>
      <td>-184.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.0</td>
      <td>-2.0</td>
      <td>56.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-4.0</td>
      <td>12.0</td>
      <td>-224.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>9.0</td>
      <td>1.0</td>
      <td>61.0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>2.0</td>
      <td>15.0</td>
      <td>-296.0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>10.0</td>
      <td>7.0</td>
      <td>-40.0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>-9.0</td>
      <td>-10.0</td>
      <td>281.0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>-8.0</td>
      <td>0.0</td>
      <td>64.0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>-0.0</td>
      <td>-5.0</td>
      <td>100.0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>-0.0</td>
      <td>-3.0</td>
      <td>60.0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>1.0</td>
      <td>2.0</td>
      <td>-39.0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>9.0</td>
      <td>4.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>6.0</td>
      <td>10.0</td>
      <td>-164.0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>9.0</td>
      <td>3.0</td>
      <td>21.0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>2.0</td>
      <td>3.0</td>
      <td>-56.0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>-2.0</td>
      <td>-14.0</td>
      <td>284.0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>-4.0</td>
      <td>-8.0</td>
      <td>176.0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>7.0</td>
      <td>11.0</td>
      <td>-171.0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>-0.0</td>
      <td>-0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>



### HyperOpt를 이용한 XGBoost 하이퍼 파라미터 최적화

#### <실습>


```python
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

dataset = load_breast_cancer()

cancer_df = pd.DataFrame(data=dataset.data, columns=dataset.feature_names)
cancer_df['target']= dataset.target
X_features = cancer_df.iloc[:, :-1]
y_label = cancer_df.iloc[:, -1]

# 전체 데이터 중 80%는 학습용 데이터, 20%는 테스트용 데이터 추출
X_train, X_test, y_train, y_test=train_test_split(X_features, y_label,
                                         test_size=0.2, random_state=156 )

# 학습 데이터를 다시 학습과 검증 데이터로 분리 
X_tr, X_val, y_tr, y_val= train_test_split(X_train, y_train,
                                         test_size=0.1, random_state=156 )
```

![](./hyperopt.png)


```python
from hyperopt import hp

# max_depth는 5에서 20까지 1간격으로, min_child_weight는 1에서 2까지 1간격으로
# colsample_bytree는 0.5에서 1사이, learning_rate는 0.01에서 0.2사이 정규 분포된 값으로 검색. 
xgb_search_space = {'max_depth': hp.quniform('max_depth', 5, 20, 1),
                    'min_child_weight': hp.quniform('min_child_weight', 1, 2, 1),
                    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),
                    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1)
               }
```


```python
from sklearn.model_selection import cross_val_score
from xgboost import XGBClassifier
from hyperopt import STATUS_OK

# fmin()에서 입력된 search_space값으로 입력된 모든 값은 실수형임. 
# XGBClassifier의 정수형 하이퍼 파라미터는 정수형 변환을 해줘야 함. 
# 정확도는 높은 수록 더 좋은 수치임. -1* 정확도를 곱해서 큰 정확도 값일 수록 최소가 되도록 변환
def objective_func(search_space):
    # 수행 시간 절약을 위해 n_estimators는 100으로 축소
    xgb_clf = XGBClassifier(n_estimators=100, max_depth=int(search_space['max_depth']),
                            min_child_weight=int(search_space['min_child_weight']),
                            learning_rate=search_space['learning_rate'],
                            colsample_bytree=search_space['colsample_bytree'], 
                            eval_metric='logloss')
    
    accuracy = cross_val_score(xgb_clf, X_train, y_train, scoring='accuracy', cv=3)
        
    # accuracy는 cv=3 개수만큼의 정확도 결과를 가지므로 이를 평균해서 반환하되 -1을 곱해줌. 
    return {'loss':-1 * np.mean(accuracy), 'status': STATUS_OK}
```


```python
from hyperopt import fmin, tpe, Trials

trial_val = Trials()
best = fmin(fn=objective_func,
            space=xgb_search_space,
            algo=tpe.suggest,
            max_evals=50, # 최대 반복 횟수를 지정합니다.
            trials=trial_val, rstate=np.random.default_rng(seed=9))
print('best:', best)
```

    100%|███████████████████████████████████████████████| 50/50 [00:29<00:00,  1.71trial/s, best loss: -0.9670616939700244]
    best: {'colsample_bytree': 0.5424149213362504, 'learning_rate': 0.12601372924444681, 'max_depth': 17.0, 'min_child_weight': 2.0}
    


```python
print('colsample_bytree:{0}, learning_rate:{1}, max_depth:{2}, min_child_weight:{3}'.format(
                        round(best['colsample_bytree'], 5), round(best['learning_rate'], 5),
                        int(best['max_depth']), int(best['min_child_weight'])))
```

    colsample_bytree:0.54241, learning_rate:0.12601, max_depth:17, min_child_weight:2
    


```python
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import f1_score, roc_auc_score

def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix( y_test, pred)
    accuracy = accuracy_score(y_test , pred)
    precision = precision_score(y_test , pred)
    recall = recall_score(y_test , pred)
    f1 = f1_score(y_test,pred)
    # ROC-AUC 추가 
    roc_auc = roc_auc_score(y_test, pred_proba)
    print('오차 행렬')
    print(confusion)
    # ROC-AUC print 추가
    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\
    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))
```


```python
xgb_wrapper = XGBClassifier(n_estimators=400, learning_rate=round(best['learning_rate'], 5), 
                            max_depth=int(best['max_depth']), min_child_weight=int(best['min_child_weight']),
                            colsample_bytree=round(best['colsample_bytree'], 5)
                           )

evals = [(X_tr, y_tr), (X_val, y_val)]
xgb_wrapper.fit(X_tr, y_tr, early_stopping_rounds=50, eval_metric='logloss', 
                eval_set=evals, verbose=True)

preds = xgb_wrapper.predict(X_test)
pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1]

get_clf_eval(y_test, preds, pred_proba)
```

    [0]	validation_0-logloss:0.58942	validation_1-logloss:0.62048
    [1]	validation_0-logloss:0.50801	validation_1-logloss:0.55913
    [2]	validation_0-logloss:0.44160	validation_1-logloss:0.50928
    [3]	validation_0-logloss:0.38734	validation_1-logloss:0.46815
    [4]	validation_0-logloss:0.34224	validation_1-logloss:0.43913
    [5]	validation_0-logloss:0.30425	validation_1-logloss:0.41570
    [6]	validation_0-logloss:0.27178	validation_1-logloss:0.38953
    [7]	validation_0-logloss:0.24503	validation_1-logloss:0.37317
    [8]	validation_0-logloss:0.22050	validation_1-logloss:0.35628
    [9]	validation_0-logloss:0.19873	validation_1-logloss:0.33798
    [10]	validation_0-logloss:0.17945	validation_1-logloss:0.32463
    [11]	validation_0-logloss:0.16354	validation_1-logloss:0.31384
    [12]	validation_0-logloss:0.15032	validation_1-logloss:0.30607
    [13]	validation_0-logloss:0.13813	validation_1-logloss:0.30143
    [14]	validation_0-logloss:0.12798	validation_1-logloss:0.29513
    [15]	validation_0-logloss:0.11926	validation_1-logloss:0.28891
    [16]	validation_0-logloss:0.11111	validation_1-logloss:0.28290
    [17]	validation_0-logloss:0.10351	validation_1-logloss:0.27835
    [18]	validation_0-logloss:0.09474	validation_1-logloss:0.27295
    [19]	validation_0-logloss:0.08922	validation_1-logloss:0.27215
    [20]	validation_0-logloss:0.08406	validation_1-logloss:0.27168
    [21]	validation_0-logloss:0.07892	validation_1-logloss:0.27093
    [22]	validation_0-logloss:0.07355	validation_1-logloss:0.26561
    [23]	validation_0-logloss:0.06976	validation_1-logloss:0.26670
    [24]	validation_0-logloss:0.06569	validation_1-logloss:0.26871
    [25]	validation_0-logloss:0.06220	validation_1-logloss:0.26484
    [26]	validation_0-logloss:0.05946	validation_1-logloss:0.26604
    [27]	validation_0-logloss:0.05688	validation_1-logloss:0.26836
    [28]	validation_0-logloss:0.05457	validation_1-logloss:0.26746
    [29]	validation_0-logloss:0.05161	validation_1-logloss:0.26410
    [30]	validation_0-logloss:0.04900	validation_1-logloss:0.26314
    [31]	validation_0-logloss:0.04658	validation_1-logloss:0.26210
    [32]	validation_0-logloss:0.04442	validation_1-logloss:0.25918
    [33]	validation_0-logloss:0.04293	validation_1-logloss:0.25575
    [34]	validation_0-logloss:0.04149	validation_1-logloss:0.25681
    [35]	validation_0-logloss:0.03991	validation_1-logloss:0.25727
    [36]	validation_0-logloss:0.03849	validation_1-logloss:0.25710
    [37]	validation_0-logloss:0.03702	validation_1-logloss:0.25274
    [38]	validation_0-logloss:0.03552	validation_1-logloss:0.25427
    [39]	validation_0-logloss:0.03441	validation_1-logloss:0.25366
    [40]	validation_0-logloss:0.03335	validation_1-logloss:0.25410
    [41]	validation_0-logloss:0.03259	validation_1-logloss:0.25415
    [42]	validation_0-logloss:0.03172	validation_1-logloss:0.25421
    [43]	validation_0-logloss:0.03110	validation_1-logloss:0.25265
    [44]	validation_0-logloss:0.03025	validation_1-logloss:0.24974
    [45]	validation_0-logloss:0.02961	validation_1-logloss:0.25033
    [46]	validation_0-logloss:0.02904	validation_1-logloss:0.24858
    [47]	validation_0-logloss:0.02849	validation_1-logloss:0.25082
    [48]	validation_0-logloss:0.02802	validation_1-logloss:0.24968
    [49]	validation_0-logloss:0.02741	validation_1-logloss:0.24913
    [50]	validation_0-logloss:0.02689	validation_1-logloss:0.24924
    [51]	validation_0-logloss:0.02644	validation_1-logloss:0.24483
    [52]	validation_0-logloss:0.02593	validation_1-logloss:0.24508
    [53]	validation_0-logloss:0.02534	validation_1-logloss:0.24153
    [54]	validation_0-logloss:0.02500	validation_1-logloss:0.23781
    [55]	validation_0-logloss:0.02458	validation_1-logloss:0.23909
    [56]	validation_0-logloss:0.02422	validation_1-logloss:0.23809
    [57]	validation_0-logloss:0.02380	validation_1-logloss:0.23843
    [58]	validation_0-logloss:0.02347	validation_1-logloss:0.23802
    [59]	validation_0-logloss:0.02310	validation_1-logloss:0.23837
    [60]	validation_0-logloss:0.02275	validation_1-logloss:0.23923
    [61]	validation_0-logloss:0.02257	validation_1-logloss:0.23813
    [62]	validation_0-logloss:0.02242	validation_1-logloss:0.23983
    [63]	validation_0-logloss:0.02227	validation_1-logloss:0.23883
    [64]	validation_0-logloss:0.02186	validation_1-logloss:0.23589
    [65]	validation_0-logloss:0.02162	validation_1-logloss:0.23630
    [66]	validation_0-logloss:0.02149	validation_1-logloss:0.23795
    [67]	validation_0-logloss:0.02136	validation_1-logloss:0.23704
    [68]	validation_0-logloss:0.02123	validation_1-logloss:0.23670
    [69]	validation_0-logloss:0.02097	validation_1-logloss:0.23745
    [70]	validation_0-logloss:0.02087	validation_1-logloss:0.23739
    [71]	validation_0-logloss:0.02073	validation_1-logloss:0.23908
    [72]	validation_0-logloss:0.02062	validation_1-logloss:0.24064
    [73]	validation_0-logloss:0.02050	validation_1-logloss:0.23973
    [74]	validation_0-logloss:0.02039	validation_1-logloss:0.23972
    [75]	validation_0-logloss:0.02029	validation_1-logloss:0.23927
    [76]	validation_0-logloss:0.02019	validation_1-logloss:0.24081
    [77]	validation_0-logloss:0.02008	validation_1-logloss:0.24054
    [78]	validation_0-logloss:0.01999	validation_1-logloss:0.23844
    [79]	validation_0-logloss:0.01989	validation_1-logloss:0.23762
    [80]	validation_0-logloss:0.01979	validation_1-logloss:0.23914
    [81]	validation_0-logloss:0.01969	validation_1-logloss:0.23907
    [82]	validation_0-logloss:0.01960	validation_1-logloss:0.23830
    [83]	validation_0-logloss:0.01952	validation_1-logloss:0.23866
    [84]	validation_0-logloss:0.01943	validation_1-logloss:0.23841
    [85]	validation_0-logloss:0.01933	validation_1-logloss:0.23992
    [86]	validation_0-logloss:0.01925	validation_1-logloss:0.23917
    [87]	validation_0-logloss:0.01916	validation_1-logloss:0.24061
    [88]	validation_0-logloss:0.01908	validation_1-logloss:0.23862
    [89]	validation_0-logloss:0.01900	validation_1-logloss:0.23858
    [90]	validation_0-logloss:0.01892	validation_1-logloss:0.23843
    [91]	validation_0-logloss:0.01884	validation_1-logloss:0.23837
    [92]	validation_0-logloss:0.01876	validation_1-logloss:0.23978
    [93]	validation_0-logloss:0.01868	validation_1-logloss:0.23787
    [94]	validation_0-logloss:0.01860	validation_1-logloss:0.23862
    [95]	validation_0-logloss:0.01853	validation_1-logloss:0.23770
    [96]	validation_0-logloss:0.01845	validation_1-logloss:0.23767
    [97]	validation_0-logloss:0.01838	validation_1-logloss:0.23840
    [98]	validation_0-logloss:0.01831	validation_1-logloss:0.23817
    [99]	validation_0-logloss:0.01824	validation_1-logloss:0.23950
    [100]	validation_0-logloss:0.01817	validation_1-logloss:0.23945
    [101]	validation_0-logloss:0.01809	validation_1-logloss:0.23848
    [102]	validation_0-logloss:0.01802	validation_1-logloss:0.23668
    [103]	validation_0-logloss:0.01795	validation_1-logloss:0.23704
    [104]	validation_0-logloss:0.01789	validation_1-logloss:0.23536
    [105]	validation_0-logloss:0.01781	validation_1-logloss:0.23465
    [106]	validation_0-logloss:0.01774	validation_1-logloss:0.23536
    [107]	validation_0-logloss:0.01768	validation_1-logloss:0.23514
    [108]	validation_0-logloss:0.01762	validation_1-logloss:0.23447
    [109]	validation_0-logloss:0.01756	validation_1-logloss:0.23281
    [110]	validation_0-logloss:0.01749	validation_1-logloss:0.23447
    [111]	validation_0-logloss:0.01743	validation_1-logloss:0.23443
    [112]	validation_0-logloss:0.01737	validation_1-logloss:0.23512
    [113]	validation_0-logloss:0.01731	validation_1-logloss:0.23465
    [114]	validation_0-logloss:0.01725	validation_1-logloss:0.23379
    [115]	validation_0-logloss:0.01719	validation_1-logloss:0.23446
    [116]	validation_0-logloss:0.01714	validation_1-logloss:0.23443
    [117]	validation_0-logloss:0.01708	validation_1-logloss:0.23360
    [118]	validation_0-logloss:0.01702	validation_1-logloss:0.23195
    [119]	validation_0-logloss:0.01696	validation_1-logloss:0.23262
    [120]	validation_0-logloss:0.01691	validation_1-logloss:0.23104
    [121]	validation_0-logloss:0.01685	validation_1-logloss:0.23145
    [122]	validation_0-logloss:0.01680	validation_1-logloss:0.23143
    [123]	validation_0-logloss:0.01675	validation_1-logloss:0.23131
    [124]	validation_0-logloss:0.01670	validation_1-logloss:0.23119
    [125]	validation_0-logloss:0.01664	validation_1-logloss:0.22965
    [126]	validation_0-logloss:0.01659	validation_1-logloss:0.23120
    [127]	validation_0-logloss:0.01654	validation_1-logloss:0.23119
    [128]	validation_0-logloss:0.01649	validation_1-logloss:0.23110
    [129]	validation_0-logloss:0.01644	validation_1-logloss:0.22957
    [130]	validation_0-logloss:0.01639	validation_1-logloss:0.22934
    [131]	validation_0-logloss:0.01634	validation_1-logloss:0.22987
    [132]	validation_0-logloss:0.01629	validation_1-logloss:0.22927
    [133]	validation_0-logloss:0.01624	validation_1-logloss:0.23076
    [134]	validation_0-logloss:0.01620	validation_1-logloss:0.23030
    [135]	validation_0-logloss:0.01615	validation_1-logloss:0.22891
    [136]	validation_0-logloss:0.01610	validation_1-logloss:0.22883
    [137]	validation_0-logloss:0.01606	validation_1-logloss:0.22882
    [138]	validation_0-logloss:0.01602	validation_1-logloss:0.22876
    [139]	validation_0-logloss:0.01597	validation_1-logloss:0.22734
    [140]	validation_0-logloss:0.01592	validation_1-logloss:0.22881
    [141]	validation_0-logloss:0.01588	validation_1-logloss:0.22935
    [142]	validation_0-logloss:0.01583	validation_1-logloss:0.22880
    [143]	validation_0-logloss:0.01579	validation_1-logloss:0.22856
    [144]	validation_0-logloss:0.01575	validation_1-logloss:0.22725
    [145]	validation_0-logloss:0.01571	validation_1-logloss:0.22739
    [146]	validation_0-logloss:0.01567	validation_1-logloss:0.22724
    [147]	validation_0-logloss:0.01562	validation_1-logloss:0.22777
    [148]	validation_0-logloss:0.01558	validation_1-logloss:0.22726
    [149]	validation_0-logloss:0.01555	validation_1-logloss:0.22721
    [150]	validation_0-logloss:0.01551	validation_1-logloss:0.22697
    [151]	validation_0-logloss:0.01546	validation_1-logloss:0.22645
    [152]	validation_0-logloss:0.01543	validation_1-logloss:0.22782
    [153]	validation_0-logloss:0.01539	validation_1-logloss:0.22790
    [154]	validation_0-logloss:0.01535	validation_1-logloss:0.22665
    [155]	validation_0-logloss:0.01531	validation_1-logloss:0.22680
    [156]	validation_0-logloss:0.01528	validation_1-logloss:0.22732
    [157]	validation_0-logloss:0.01524	validation_1-logloss:0.22709
    [158]	validation_0-logloss:0.01520	validation_1-logloss:0.22659
    [159]	validation_0-logloss:0.01516	validation_1-logloss:0.22616
    [160]	validation_0-logloss:0.01513	validation_1-logloss:0.22631
    [161]	validation_0-logloss:0.01509	validation_1-logloss:0.22510
    [162]	validation_0-logloss:0.01506	validation_1-logloss:0.22562
    [163]	validation_0-logloss:0.01502	validation_1-logloss:0.22539
    [164]	validation_0-logloss:0.01499	validation_1-logloss:0.22671
    [165]	validation_0-logloss:0.01495	validation_1-logloss:0.22625
    [166]	validation_0-logloss:0.01492	validation_1-logloss:0.22585
    [167]	validation_0-logloss:0.01489	validation_1-logloss:0.22583
    [168]	validation_0-logloss:0.01485	validation_1-logloss:0.22562
    [169]	validation_0-logloss:0.01482	validation_1-logloss:0.22520
    [170]	validation_0-logloss:0.01479	validation_1-logloss:0.22570
    [171]	validation_0-logloss:0.01476	validation_1-logloss:0.22587
    [172]	validation_0-logloss:0.01472	validation_1-logloss:0.22466
    [173]	validation_0-logloss:0.01469	validation_1-logloss:0.22592
    [174]	validation_0-logloss:0.01466	validation_1-logloss:0.22599
    [175]	validation_0-logloss:0.01463	validation_1-logloss:0.22556
    [176]	validation_0-logloss:0.01460	validation_1-logloss:0.22535
    [177]	validation_0-logloss:0.01457	validation_1-logloss:0.22655
    [178]	validation_0-logloss:0.01454	validation_1-logloss:0.22674
    [179]	validation_0-logloss:0.01451	validation_1-logloss:0.22565
    [180]	validation_0-logloss:0.01448	validation_1-logloss:0.22565
    [181]	validation_0-logloss:0.01445	validation_1-logloss:0.22526
    [182]	validation_0-logloss:0.01442	validation_1-logloss:0.22545
    [183]	validation_0-logloss:0.01439	validation_1-logloss:0.22504
    [184]	validation_0-logloss:0.01436	validation_1-logloss:0.22554
    [185]	validation_0-logloss:0.01433	validation_1-logloss:0.22533
    [186]	validation_0-logloss:0.01431	validation_1-logloss:0.22426
    [187]	validation_0-logloss:0.01428	validation_1-logloss:0.22545
    [188]	validation_0-logloss:0.01425	validation_1-logloss:0.22563
    [189]	validation_0-logloss:0.01422	validation_1-logloss:0.22525
    [190]	validation_0-logloss:0.01419	validation_1-logloss:0.22504
    [191]	validation_0-logloss:0.01417	validation_1-logloss:0.22523
    [192]	validation_0-logloss:0.01414	validation_1-logloss:0.22529
    [193]	validation_0-logloss:0.01411	validation_1-logloss:0.22492
    [194]	validation_0-logloss:0.01409	validation_1-logloss:0.22472
    [195]	validation_0-logloss:0.01406	validation_1-logloss:0.22589
    [196]	validation_0-logloss:0.01403	validation_1-logloss:0.22595
    [197]	validation_0-logloss:0.01401	validation_1-logloss:0.22646
    [198]	validation_0-logloss:0.01399	validation_1-logloss:0.22665
    [199]	validation_0-logloss:0.01396	validation_1-logloss:0.22628
    [200]	validation_0-logloss:0.01393	validation_1-logloss:0.22609
    [201]	validation_0-logloss:0.01391	validation_1-logloss:0.22572
    [202]	validation_0-logloss:0.01388	validation_1-logloss:0.22536
    [203]	validation_0-logloss:0.01386	validation_1-logloss:0.22586
    [204]	validation_0-logloss:0.01384	validation_1-logloss:0.22568
    [205]	validation_0-logloss:0.01381	validation_1-logloss:0.22678
    [206]	validation_0-logloss:0.01379	validation_1-logloss:0.22642
    [207]	validation_0-logloss:0.01377	validation_1-logloss:0.22690
    [208]	validation_0-logloss:0.01375	validation_1-logloss:0.22710
    [209]	validation_0-logloss:0.01373	validation_1-logloss:0.22676
    [210]	validation_0-logloss:0.01371	validation_1-logloss:0.22643
    [211]	validation_0-logloss:0.01368	validation_1-logloss:0.22624
    [212]	validation_0-logloss:0.01366	validation_1-logloss:0.22727
    [213]	validation_0-logloss:0.01364	validation_1-logloss:0.22693
    [214]	validation_0-logloss:0.01362	validation_1-logloss:0.22675
    [215]	validation_0-logloss:0.01360	validation_1-logloss:0.22644
    [216]	validation_0-logloss:0.01357	validation_1-logloss:0.22665
    [217]	validation_0-logloss:0.01355	validation_1-logloss:0.22685
    [218]	validation_0-logloss:0.01353	validation_1-logloss:0.22653
    [219]	validation_0-logloss:0.01351	validation_1-logloss:0.22622
    [220]	validation_0-logloss:0.01350	validation_1-logloss:0.22642
    [221]	validation_0-logloss:0.01348	validation_1-logloss:0.22612
    [222]	validation_0-logloss:0.01346	validation_1-logloss:0.22594
    [223]	validation_0-logloss:0.01344	validation_1-logloss:0.22641
    [224]	validation_0-logloss:0.01342	validation_1-logloss:0.22623
    [225]	validation_0-logloss:0.01340	validation_1-logloss:0.22668
    [226]	validation_0-logloss:0.01338	validation_1-logloss:0.22766
    [227]	validation_0-logloss:0.01336	validation_1-logloss:0.22736
    [228]	validation_0-logloss:0.01334	validation_1-logloss:0.22706
    [229]	validation_0-logloss:0.01333	validation_1-logloss:0.22727
    [230]	validation_0-logloss:0.01331	validation_1-logloss:0.22709
    [231]	validation_0-logloss:0.01329	validation_1-logloss:0.22694
    [232]	validation_0-logloss:0.01327	validation_1-logloss:0.22706
    [233]	validation_0-logloss:0.01325	validation_1-logloss:0.22676
    [234]	validation_0-logloss:0.01324	validation_1-logloss:0.22773
    [235]	validation_0-logloss:0.01322	validation_1-logloss:0.22743
    오차 행렬
    [[35  2]
     [ 2 75]]
    정확도: 0.9649, 정밀도: 0.9740, 재현율: 0.9740,    F1: 0.9740, AUC:0.9944
    


```python
losses = [loss_dict['loss'] for loss_dict in trial_val.results]
result_df = pd.DataFrame({'max_depth': trial_val.vals['max_depth'],
                          'min_child_weight': trial_val.vals['min_child_weight'],
                          'colsample_bytree': trial_val.vals['colsample_bytree'],
                          'learning_rate': trial_val.vals['learning_rate'],
                          'losses': losses
                         }
                        )
result_df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>max_depth</th>
      <th>min_child_weight</th>
      <th>colsample_bytree</th>
      <th>learning_rate</th>
      <th>losses</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>19.0</td>
      <td>2.0</td>
      <td>0.585235</td>
      <td>0.033688</td>
      <td>-0.947296</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5.0</td>
      <td>2.0</td>
      <td>0.727186</td>
      <td>0.105956</td>
      <td>-0.960483</td>
    </tr>
    <tr>
      <th>2</th>
      <td>6.0</td>
      <td>2.0</td>
      <td>0.959945</td>
      <td>0.154804</td>
      <td>-0.958290</td>
    </tr>
    <tr>
      <th>3</th>
      <td>6.0</td>
      <td>2.0</td>
      <td>0.950012</td>
      <td>0.120686</td>
      <td>-0.960468</td>
    </tr>
    <tr>
      <th>4</th>
      <td>16.0</td>
      <td>2.0</td>
      <td>0.674336</td>
      <td>0.142392</td>
      <td>-0.962661</td>
    </tr>
    <tr>
      <th>5</th>
      <td>8.0</td>
      <td>2.0</td>
      <td>0.863774</td>
      <td>0.106579</td>
      <td>-0.958275</td>
    </tr>
    <tr>
      <th>6</th>
      <td>14.0</td>
      <td>2.0</td>
      <td>0.957521</td>
      <td>0.079111</td>
      <td>-0.956097</td>
    </tr>
    <tr>
      <th>7</th>
      <td>19.0</td>
      <td>2.0</td>
      <td>0.695018</td>
      <td>0.095213</td>
      <td>-0.960468</td>
    </tr>
    <tr>
      <th>8</th>
      <td>9.0</td>
      <td>2.0</td>
      <td>0.684442</td>
      <td>0.147520</td>
      <td>-0.962661</td>
    </tr>
    <tr>
      <th>9</th>
      <td>8.0</td>
      <td>1.0</td>
      <td>0.592116</td>
      <td>0.081179</td>
      <td>-0.956097</td>
    </tr>
    <tr>
      <th>10</th>
      <td>6.0</td>
      <td>2.0</td>
      <td>0.614798</td>
      <td>0.076255</td>
      <td>-0.956082</td>
    </tr>
    <tr>
      <th>11</th>
      <td>7.0</td>
      <td>2.0</td>
      <td>0.776738</td>
      <td>0.089624</td>
      <td>-0.960468</td>
    </tr>
    <tr>
      <th>12</th>
      <td>8.0</td>
      <td>2.0</td>
      <td>0.514772</td>
      <td>0.092214</td>
      <td>-0.958275</td>
    </tr>
    <tr>
      <th>13</th>
      <td>19.0</td>
      <td>1.0</td>
      <td>0.949783</td>
      <td>0.083983</td>
      <td>-0.949474</td>
    </tr>
    <tr>
      <th>14</th>
      <td>10.0</td>
      <td>1.0</td>
      <td>0.926121</td>
      <td>0.112477</td>
      <td>-0.949489</td>
    </tr>
    <tr>
      <th>15</th>
      <td>6.0</td>
      <td>2.0</td>
      <td>0.570990</td>
      <td>0.064663</td>
      <td>-0.958290</td>
    </tr>
    <tr>
      <th>16</th>
      <td>7.0</td>
      <td>2.0</td>
      <td>0.884549</td>
      <td>0.042766</td>
      <td>-0.949489</td>
    </tr>
    <tr>
      <th>17</th>
      <td>18.0</td>
      <td>2.0</td>
      <td>0.548302</td>
      <td>0.184028</td>
      <td>-0.962647</td>
    </tr>
    <tr>
      <th>18</th>
      <td>6.0</td>
      <td>2.0</td>
      <td>0.910278</td>
      <td>0.133006</td>
      <td>-0.960468</td>
    </tr>
    <tr>
      <th>19</th>
      <td>9.0</td>
      <td>2.0</td>
      <td>0.532501</td>
      <td>0.091771</td>
      <td>-0.964869</td>
    </tr>
    <tr>
      <th>20</th>
      <td>15.0</td>
      <td>1.0</td>
      <td>0.644890</td>
      <td>0.189043</td>
      <td>-0.958275</td>
    </tr>
    <tr>
      <th>21</th>
      <td>11.0</td>
      <td>1.0</td>
      <td>0.780915</td>
      <td>0.154057</td>
      <td>-0.960468</td>
    </tr>
    <tr>
      <th>22</th>
      <td>11.0</td>
      <td>2.0</td>
      <td>0.510122</td>
      <td>0.169793</td>
      <td>-0.960483</td>
    </tr>
    <tr>
      <th>23</th>
      <td>16.0</td>
      <td>1.0</td>
      <td>0.822165</td>
      <td>0.054728</td>
      <td>-0.947296</td>
    </tr>
    <tr>
      <th>24</th>
      <td>13.0</td>
      <td>2.0</td>
      <td>0.647444</td>
      <td>0.011072</td>
      <td>-0.936316</td>
    </tr>
    <tr>
      <th>25</th>
      <td>17.0</td>
      <td>2.0</td>
      <td>0.542415</td>
      <td>0.126014</td>
      <td>-0.967062</td>
    </tr>
    <tr>
      <th>26</th>
      <td>17.0</td>
      <td>1.0</td>
      <td>0.538160</td>
      <td>0.128161</td>
      <td>-0.962676</td>
    </tr>
    <tr>
      <th>27</th>
      <td>12.0</td>
      <td>2.0</td>
      <td>0.506463</td>
      <td>0.010147</td>
      <td>-0.940717</td>
    </tr>
    <tr>
      <th>28</th>
      <td>13.0</td>
      <td>2.0</td>
      <td>0.616162</td>
      <td>0.170540</td>
      <td>-0.958275</td>
    </tr>
    <tr>
      <th>29</th>
      <td>20.0</td>
      <td>2.0</td>
      <td>0.564500</td>
      <td>0.025787</td>
      <td>-0.942924</td>
    </tr>
    <tr>
      <th>30</th>
      <td>10.0</td>
      <td>2.0</td>
      <td>0.733826</td>
      <td>0.058339</td>
      <td>-0.951696</td>
    </tr>
    <tr>
      <th>31</th>
      <td>14.0</td>
      <td>2.0</td>
      <td>0.501102</td>
      <td>0.119548</td>
      <td>-0.967062</td>
    </tr>
    <tr>
      <th>32</th>
      <td>15.0</td>
      <td>2.0</td>
      <td>0.597853</td>
      <td>0.170319</td>
      <td>-0.960454</td>
    </tr>
    <tr>
      <th>33</th>
      <td>17.0</td>
      <td>1.0</td>
      <td>0.501951</td>
      <td>0.113862</td>
      <td>-0.962676</td>
    </tr>
    <tr>
      <th>34</th>
      <td>14.0</td>
      <td>2.0</td>
      <td>0.709170</td>
      <td>0.135741</td>
      <td>-0.960454</td>
    </tr>
    <tr>
      <th>35</th>
      <td>18.0</td>
      <td>2.0</td>
      <td>0.999433</td>
      <td>0.199366</td>
      <td>-0.960454</td>
    </tr>
    <tr>
      <th>36</th>
      <td>15.0</td>
      <td>2.0</td>
      <td>0.651538</td>
      <td>0.122986</td>
      <td>-0.960454</td>
    </tr>
    <tr>
      <th>37</th>
      <td>20.0</td>
      <td>2.0</td>
      <td>0.839988</td>
      <td>0.101882</td>
      <td>-0.958275</td>
    </tr>
    <tr>
      <th>38</th>
      <td>16.0</td>
      <td>2.0</td>
      <td>0.765179</td>
      <td>0.149996</td>
      <td>-0.956053</td>
    </tr>
    <tr>
      <th>39</th>
      <td>14.0</td>
      <td>1.0</td>
      <td>0.613403</td>
      <td>0.139308</td>
      <td>-0.958290</td>
    </tr>
    <tr>
      <th>40</th>
      <td>17.0</td>
      <td>2.0</td>
      <td>0.666513</td>
      <td>0.102078</td>
      <td>-0.962661</td>
    </tr>
    <tr>
      <th>41</th>
      <td>18.0</td>
      <td>2.0</td>
      <td>0.559546</td>
      <td>0.069568</td>
      <td>-0.960483</td>
    </tr>
    <tr>
      <th>42</th>
      <td>12.0</td>
      <td>2.0</td>
      <td>0.527415</td>
      <td>0.161834</td>
      <td>-0.967062</td>
    </tr>
    <tr>
      <th>43</th>
      <td>12.0</td>
      <td>2.0</td>
      <td>0.588290</td>
      <td>0.160257</td>
      <td>-0.962661</td>
    </tr>
    <tr>
      <th>44</th>
      <td>19.0</td>
      <td>1.0</td>
      <td>0.804978</td>
      <td>0.116651</td>
      <td>-0.951696</td>
    </tr>
    <tr>
      <th>45</th>
      <td>13.0</td>
      <td>2.0</td>
      <td>0.696878</td>
      <td>0.145955</td>
      <td>-0.964854</td>
    </tr>
    <tr>
      <th>46</th>
      <td>11.0</td>
      <td>2.0</td>
      <td>0.524901</td>
      <td>0.181720</td>
      <td>-0.964854</td>
    </tr>
    <tr>
      <th>47</th>
      <td>10.0</td>
      <td>2.0</td>
      <td>0.725896</td>
      <td>0.198962</td>
      <td>-0.964840</td>
    </tr>
    <tr>
      <th>48</th>
      <td>12.0</td>
      <td>1.0</td>
      <td>0.630900</td>
      <td>0.107408</td>
      <td>-0.960497</td>
    </tr>
    <tr>
      <th>49</th>
      <td>14.0</td>
      <td>2.0</td>
      <td>0.675242</td>
      <td>0.125260</td>
      <td>-0.960468</td>
    </tr>
  </tbody>
</table>
</div>




```python

```
