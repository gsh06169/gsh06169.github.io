---
title: "[Python 머신러닝] 04-5 GBM(Gradient Boosting Machine)"

categories: 
  - PYTHON
tags:
  - [Python, 프로그래밍, 머신러닝, 공부]

toc: true
toc_sticky: true
---

# 분류

## GBM(Gradient Boosting Machine)

### GBM의 개요 및 실습

#### 부스팅(Boosting)
- 부스팅 알고리즘은 여러 개의 약한 학습기(weak learner)를 순차적으로 학습-예측하면서 잘못 예측한 데이터나 학습 트리에 가중치 부여를 통해 오류를 개선해 나가면서 학습하는 방식이다.
- 부스팅의 대표적인 구현은 AdaBoost(Adaptive boosting)와 그래디어튼 부스트가 있다.


#### 에이다 부스팅의 학습/예측 프로세스

![IMG_7338](https://github.com/gsh06169/gsh06169/assets/150469460/d614e3f2-9f8b-431d-ae61-8805396c694e)

![IMG_7339](https://github.com/gsh06169/gsh06169/assets/150469460/b0a16d8a-8dec-479a-94a7-3d733dfa29ac)


#### GBM(Gradient Boost Machine) 개요
- GBM(Gradient Boost Machine)도 에이다 부스트와 유사하나, 가중치 업데이트를 경사 하강법(Gradient Descent)을 이용하는 것이 큰 차이이다.
- 오류 값은 실제 값 - 예측값이다. <br> 분류의 실제 결괏값을 $y$, 피처를 $x1, x2, ..., xn$, 그리고 이 피처에 기반한 예측 함수를 $F(x)$라고 하면 오류식 $h(x) = y - F(x)$이 된다. <br> 이 오류식 $h(x) = y - F(x)$를 최소화하는 방향성을 가지고 반복적으로 가중치 값을 업데이트하는 것이 경사 하강법(Gradient Descent)이다.
- 경사 하강법은 반복 수행을 통해 오류를 최소화할 수 있도록 가중치의 업데이트 값을 도출하는 기법으로서 머신러닝에서 중요한 기법 중 하나이다.



#### <실습>

```python
from sklearn.ensemble import GradientBoostingClassifier
import time
import warnings
warnings.filterwarnings('ignore')

X_train, X_test, y_train, y_test = get_human_dataset()

# GBM 수행 시간 측정을 위함. 시작 시간 설정.
start_time = time.time()

gb_clf = GradientBoostingClassifier(random_state=0)
gb_clf.fit(X_train , y_train)
gb_pred = gb_clf.predict(X_test)
gb_accuracy = accuracy_score(y_test, gb_pred)

print('GBM 정확도: {0:.4f}'.format(gb_accuracy))
print("GBM 수행 시간: {0:.1f} 초 ".format(time.time() - start_time))

```

    GBM 정확도: 0.9389
    GBM 수행 시간: 540.1 초 
    

### GBM 하이퍼 파라미터 소개

사이킷런은 GBM 분류를 위해 GradientBoostingClassifier 클래스를 제공한다.

- loss: 경사 하강법에서 사용할 비용 함수를 지정한다. <br> 특별한 이유가 없으면 기본값인 'deviance'를 그래도 적용한다.
- learning_rate: GBM이 학습을 진행할 때마다 적용하는 학습률이다. <br> Weak learner가 순차적으로 오류 값을 보정해 나가는 데 적용하는 계수이다. <br> 0~1 사이의 값을 지정할 수 있으며 기본값은 0.1이다. <br> 너무 작은 값을 적용하면 업데이트 되는 값이 작아져서 최소 오류 값을 찾아 예측 성능이 높아질 가능성이 높다. <br> 하지만 많은 weak learner는 순차적인 반복이 필요해서 수행 시간이 오래 걸리고, 또 너무 작게 설정하면 모든 weak learner의 반복이 완료돼도 최소 오류 값을 찾지 못할 수 있다. <br> 반대로 큰 값을 적용하면 최소 오류 값을 찾지 못하고 그냥 지나쳐 버려 예측 성능이 떨어질 가능성이 높아지지만, 빠른 수행이 가능하다.
- n_estimators: weak learner의 개수이다. <br> weak learner가 순차적으로 오류를 보정하므로 개수가 많을 수록 예측 성능이 일정 수준까지는 좋아질 수 있다. <br> 하지만 개수가 많을수록 수행 시간이 오래 걸린다. 기본값은 100이다.
- subsample: weak learner가 학습에 사용하는 데이터의 샘플링 비율이다. <br> 기본값은 1이며, 이는 전체 학습 데이터를 기반으로 학습한다는 의미이다. (0.5이면 학습 데이터의 50%) <br> 과적합이 염려되는 경우 subsample을 1보다 작은 값으로 설정한다.


#### <실습>

```python
### 사이킷런이 1.X로 업그레이드 되며서 GBM의 학습 속도가 현저하게 저하되는 문제가 오히려 발생합니다. 
### 아래는 수행 시간이 오래 걸리므로 참고용으로만 사용하시면 좋을 것 같습니다. 
from sklearn.model_selection import GridSearchCV

params = {
    'n_estimators':[100, 500],
    'learning_rate' : [ 0.05, 0.1]
}
grid_cv = GridSearchCV(gb_clf , param_grid=params , cv=2 ,verbose=1)
grid_cv.fit(X_train , y_train)
print('최적 하이퍼 파라미터:\n', grid_cv.best_params_)
print('최고 예측 정확도: {0:.4f}'.format(grid_cv.best_score_))
```


```python
# GridSearchCV를 이용하여 최적으로 학습된 estimator로 predict 수행. 
gb_pred = grid_cv.best_estimator_.predict(X_test)
gb_accuracy = accuracy_score(y_test, gb_pred)
print('GBM 정확도: {0:.4f}'.format(gb_accuracy))
```
